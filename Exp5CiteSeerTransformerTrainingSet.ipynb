{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exp5CiteSeerTransformerTrainingSet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN8+IcAHQtEolkWZTQo8CNq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fYvxMt6P-g8e","executionInfo":{"status":"ok","timestamp":1645977946982,"user_tz":-60,"elapsed":11356,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"78c1f254-c060-4df8-f556-7d1fae221a16"},"source":["#Check the PyTorch and Cuda version\n","!python -c \"import torch; print(torch.__version__)\"\n","!python -c \"import torch; print(torch.version.cuda)\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n","11.1\n"]}]},{"cell_type":"code","metadata":{"id":"ICFCAiNp-qHq"},"source":["!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LgzQTkyL-4GJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646055317778,"user_tz":-60,"elapsed":11187,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"b9f16826-7f13-42ec-ef83-78c544a096c5"},"source":["#First data characteristics\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.transforms import NormalizeFeatures\n","\n","dataset = Planetoid(root='/tmp/CiteSeer', name='CiteSeer',transform=NormalizeFeatures())\n","\n","data = dataset[0]\n","print(data)\n","\n","print(f'Number of nodes: {data.num_nodes}')\n","print(f'Nodes features: {data.num_node_features}')\n","print(f'Number of classes: {dataset.num_classes}')\n","print(f'Number of edges: {data.num_edges}')\n","print(f'Avarage degree: {data.num_edges / data.num_nodes:.2f}')\n","print(f'Training nodes: {data.train_mask.sum()}')\n","print(f'Validation nodes: {data.val_mask.sum()}')\n","print(f'Test nodes: {data.test_mask.sum()}')\n","print(f'Isolated nodes: {data.has_isolated_nodes()}')\n","print(f'Loops: {data.has_self_loops()}')\n","print(f'Is undirected: {data.is_undirected()}')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n","Processing...\n"]},{"output_type":"stream","name":"stdout","text":["Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])\n","Number of nodes: 3327\n","Nodes features: 3703\n","Number of classes: 6\n","Number of edges: 9104\n","Avarage degree: 2.74\n","Training nodes: 120\n","Validation nodes: 500\n","Test nodes: 1000\n","Isolated nodes: True\n","Loops: False\n","Is undirected: True\n"]},{"output_type":"stream","name":"stderr","text":["Done!\n"]}]},{"cell_type":"markdown","metadata":{"id":"98tFjZzDyAF8"},"source":["**Applying self-attention to all nodes using the first 1500 nodes to train**\n","\n","Using GNNs, the prediction of each node is done using neural network layers on an aggregation of neighboring nodes. Now we are going to use the building block of a Transformer, applying self-attention to all nodes of the graph. A part of these nodes will be used for training and another for validation and test."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMvHVxzkRc4d","executionInfo":{"status":"ok","timestamp":1646055374918,"user_tz":-60,"elapsed":197,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"218fc9f5-d5e2-4a6f-de1e-acbb045fd5fc"},"source":["#Add a new batch dimension of size 1 to the tensor\n","import torch\n","\n","data1=torch.unsqueeze(data.x, 1)\n","data1.shape"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3327, 1, 3703])"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"Kzomc_siIzt3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646055393790,"user_tz":-60,"elapsed":9425,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"0fc1181e-a470-4fd5-df18-ec117adc3d7d"},"source":["#Convert the input features dimension to the model features\n","#Apply self-attention using PyTorch MultiheadAttention\n","#Transform the encoder output to the output dimension\n","#Set up the model and tensors in cuda\n","import torch.nn\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","\n","class Self_Att(torch.nn.Module):\n","    def __init__(self):\n","        super(Self_Att, self).__init__()\n","        torch.manual_seed(12345)\n","        self.lin1 = Linear(3703,120)\n","        self.attention1 = torch.nn.MultiheadAttention(120,6)\n","        self.lin2 = Linear(120, dataset.num_classes)\n","\n","    def forward(self,x):\n","        x = self.lin1(x)\n","        x = F.dropout(x, p=0.8, training=self.training)\n","        x, attn_output_weights = self.attention1(x,x,x)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.lin2(x)\n","        x=torch.squeeze(x)#Remove the dimension of size 1\n","        return x,attn_output_weights\n","\n","use_cuda = True\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","\n","model = Self_Att()\n","model.cuda()\n","\n","data=data.cuda()\n","data1=data1.cuda()\n","\n","print(model)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Self_Att(\n","  (lin1): Linear(in_features=3703, out_features=120, bias=True)\n","  (attention1): MultiheadAttention(\n","    (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n","  )\n","  (lin2): Linear(in_features=120, out_features=6, bias=True)\n",")\n"]}]},{"cell_type":"code","metadata":{"id":"RwsWAdtMVjej","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1646055589283,"user_tz":-60,"elapsed":12335,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"32fc7112-280c-4932-8271-899cbdcf3530"},"source":["#For training, the cross entropy loss combines LogSoftmax and NLLLoss in one single class\n","#input is expected to contain raw, unnormalized scores for each class\n","#target a class index in the range for each value of a 1D tensor of size minibatch\n","#For testing, compares the class with highest probability with the labels and counts the correct predictions\n","from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out,attn_output_weights = model(data1)  # Perform a single forward pass.\n","      loss = criterion(out[0:1500], data.y[0:1500])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss,attn_output_weights\n","\n","def test():\n","      model.eval()\n","      out,attn_output_weights = model(data1)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc,attn_output_weights\n","\n","for epoch in range(1, 201):\n","    loss,attn_output_weights = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n","    #print(attn_output_weights)"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 1.7458\n","Epoch: 002, Loss: 1.7432\n","Epoch: 003, Loss: 1.7429\n","Epoch: 004, Loss: 1.7431\n","Epoch: 005, Loss: 1.7432\n","Epoch: 006, Loss: 1.7429\n","Epoch: 007, Loss: 1.7424\n","Epoch: 008, Loss: 1.7431\n","Epoch: 009, Loss: 1.7424\n","Epoch: 010, Loss: 1.7435\n","Epoch: 011, Loss: 1.7428\n","Epoch: 012, Loss: 1.7427\n","Epoch: 013, Loss: 1.7428\n","Epoch: 014, Loss: 1.7422\n","Epoch: 015, Loss: 1.7427\n","Epoch: 016, Loss: 1.7428\n","Epoch: 017, Loss: 1.7423\n","Epoch: 018, Loss: 1.7427\n","Epoch: 019, Loss: 1.7425\n","Epoch: 020, Loss: 1.7431\n","Epoch: 021, Loss: 1.7424\n","Epoch: 022, Loss: 1.7425\n","Epoch: 023, Loss: 1.7428\n","Epoch: 024, Loss: 1.7427\n","Epoch: 025, Loss: 1.7425\n","Epoch: 026, Loss: 1.7427\n","Epoch: 027, Loss: 1.7427\n","Epoch: 028, Loss: 1.7425\n","Epoch: 029, Loss: 1.7424\n","Epoch: 030, Loss: 1.7434\n","Epoch: 031, Loss: 1.7430\n","Epoch: 032, Loss: 1.7433\n","Epoch: 033, Loss: 1.7424\n","Epoch: 034, Loss: 1.7429\n","Epoch: 035, Loss: 1.7427\n","Epoch: 036, Loss: 1.7429\n","Epoch: 037, Loss: 1.7427\n","Epoch: 038, Loss: 1.7426\n","Epoch: 039, Loss: 1.7427\n","Epoch: 040, Loss: 1.7431\n","Epoch: 041, Loss: 1.7428\n","Epoch: 042, Loss: 1.7425\n","Epoch: 043, Loss: 1.7428\n","Epoch: 044, Loss: 1.7428\n","Epoch: 045, Loss: 1.7432\n","Epoch: 046, Loss: 1.7431\n","Epoch: 047, Loss: 1.7424\n","Epoch: 048, Loss: 1.7428\n","Epoch: 049, Loss: 1.7424\n","Epoch: 050, Loss: 1.7428\n","Epoch: 051, Loss: 1.7423\n","Epoch: 052, Loss: 1.7426\n","Epoch: 053, Loss: 1.7425\n","Epoch: 054, Loss: 1.7429\n","Epoch: 055, Loss: 1.7431\n","Epoch: 056, Loss: 1.7435\n","Epoch: 057, Loss: 1.7422\n","Epoch: 058, Loss: 1.7426\n","Epoch: 059, Loss: 1.7433\n","Epoch: 060, Loss: 1.7432\n","Epoch: 061, Loss: 1.7430\n","Epoch: 062, Loss: 1.7428\n","Epoch: 063, Loss: 1.7426\n","Epoch: 064, Loss: 1.7429\n","Epoch: 065, Loss: 1.7430\n","Epoch: 066, Loss: 1.7423\n","Epoch: 067, Loss: 1.7427\n","Epoch: 068, Loss: 1.7428\n","Epoch: 069, Loss: 1.7430\n","Epoch: 070, Loss: 1.7430\n","Epoch: 071, Loss: 1.7421\n","Epoch: 072, Loss: 1.7430\n","Epoch: 073, Loss: 1.7425\n","Epoch: 074, Loss: 1.7427\n","Epoch: 075, Loss: 1.7434\n","Epoch: 076, Loss: 1.7425\n","Epoch: 077, Loss: 1.7432\n","Epoch: 078, Loss: 1.7428\n","Epoch: 079, Loss: 1.7428\n","Epoch: 080, Loss: 1.7424\n","Epoch: 081, Loss: 1.7427\n","Epoch: 082, Loss: 1.7421\n","Epoch: 083, Loss: 1.7430\n","Epoch: 084, Loss: 1.7427\n","Epoch: 085, Loss: 1.7427\n","Epoch: 086, Loss: 1.7428\n","Epoch: 087, Loss: 1.7429\n","Epoch: 088, Loss: 1.7432\n","Epoch: 089, Loss: 1.7428\n","Epoch: 090, Loss: 1.7424\n","Epoch: 091, Loss: 1.7430\n","Epoch: 092, Loss: 1.7427\n","Epoch: 093, Loss: 1.7431\n","Epoch: 094, Loss: 1.7435\n","Epoch: 095, Loss: 1.7427\n","Epoch: 096, Loss: 1.7424\n","Epoch: 097, Loss: 1.7425\n","Epoch: 098, Loss: 1.7430\n","Epoch: 099, Loss: 1.7428\n","Epoch: 100, Loss: 1.7426\n","Epoch: 101, Loss: 1.7426\n","Epoch: 102, Loss: 1.7433\n","Epoch: 103, Loss: 1.7423\n","Epoch: 104, Loss: 1.7425\n","Epoch: 105, Loss: 1.7430\n","Epoch: 106, Loss: 1.7429\n","Epoch: 107, Loss: 1.7428\n","Epoch: 108, Loss: 1.7429\n","Epoch: 109, Loss: 1.7425\n","Epoch: 110, Loss: 1.7428\n","Epoch: 111, Loss: 1.7433\n","Epoch: 112, Loss: 1.7426\n","Epoch: 113, Loss: 1.7432\n","Epoch: 114, Loss: 1.7428\n","Epoch: 115, Loss: 1.7427\n","Epoch: 116, Loss: 1.7429\n","Epoch: 117, Loss: 1.7429\n","Epoch: 118, Loss: 1.7426\n","Epoch: 119, Loss: 1.7429\n","Epoch: 120, Loss: 1.7429\n","Epoch: 121, Loss: 1.7424\n","Epoch: 122, Loss: 1.7428\n","Epoch: 123, Loss: 1.7426\n","Epoch: 124, Loss: 1.7427\n","Epoch: 125, Loss: 1.7427\n","Epoch: 126, Loss: 1.7425\n","Epoch: 127, Loss: 1.7427\n","Epoch: 128, Loss: 1.7429\n","Epoch: 129, Loss: 1.7425\n","Epoch: 130, Loss: 1.7427\n","Epoch: 131, Loss: 1.7430\n","Epoch: 132, Loss: 1.7429\n","Epoch: 133, Loss: 1.7427\n","Epoch: 134, Loss: 1.7427\n","Epoch: 135, Loss: 1.7426\n","Epoch: 136, Loss: 1.7428\n","Epoch: 137, Loss: 1.7430\n","Epoch: 138, Loss: 1.7429\n","Epoch: 139, Loss: 1.7430\n","Epoch: 140, Loss: 1.7426\n","Epoch: 141, Loss: 1.7425\n","Epoch: 142, Loss: 1.7427\n","Epoch: 143, Loss: 1.7427\n","Epoch: 144, Loss: 1.7426\n","Epoch: 145, Loss: 1.7428\n","Epoch: 146, Loss: 1.7428\n","Epoch: 147, Loss: 1.7429\n","Epoch: 148, Loss: 1.7425\n","Epoch: 149, Loss: 1.7427\n","Epoch: 150, Loss: 1.7427\n","Epoch: 151, Loss: 1.7422\n","Epoch: 152, Loss: 1.7425\n","Epoch: 153, Loss: 1.7425\n","Epoch: 154, Loss: 1.7433\n","Epoch: 155, Loss: 1.7426\n","Epoch: 156, Loss: 1.7431\n","Epoch: 157, Loss: 1.7424\n","Epoch: 158, Loss: 1.7424\n","Epoch: 159, Loss: 1.7430\n","Epoch: 160, Loss: 1.7424\n","Epoch: 161, Loss: 1.7426\n","Epoch: 162, Loss: 1.7431\n","Epoch: 163, Loss: 1.7426\n","Epoch: 164, Loss: 1.7428\n","Epoch: 165, Loss: 1.7427\n","Epoch: 166, Loss: 1.7426\n","Epoch: 167, Loss: 1.7426\n","Epoch: 168, Loss: 1.7433\n","Epoch: 169, Loss: 1.7427\n","Epoch: 170, Loss: 1.7424\n","Epoch: 171, Loss: 1.7429\n","Epoch: 172, Loss: 1.7428\n","Epoch: 173, Loss: 1.7429\n","Epoch: 174, Loss: 1.7432\n","Epoch: 175, Loss: 1.7426\n","Epoch: 176, Loss: 1.7424\n","Epoch: 177, Loss: 1.7432\n","Epoch: 178, Loss: 1.7429\n","Epoch: 179, Loss: 1.7426\n","Epoch: 180, Loss: 1.7429\n","Epoch: 181, Loss: 1.7430\n","Epoch: 182, Loss: 1.7425\n","Epoch: 183, Loss: 1.7428\n","Epoch: 184, Loss: 1.7427\n","Epoch: 185, Loss: 1.7426\n","Epoch: 186, Loss: 1.7427\n","Epoch: 187, Loss: 1.7429\n","Epoch: 188, Loss: 1.7427\n","Epoch: 189, Loss: 1.7425\n","Epoch: 190, Loss: 1.7430\n","Epoch: 191, Loss: 1.7430\n","Epoch: 192, Loss: 1.7427\n","Epoch: 193, Loss: 1.7426\n","Epoch: 194, Loss: 1.7430\n","Epoch: 195, Loss: 1.7423\n","Epoch: 196, Loss: 1.7424\n","Epoch: 197, Loss: 1.7428\n","Epoch: 198, Loss: 1.7432\n","Epoch: 199, Loss: 1.7425\n","Epoch: 200, Loss: 1.7430\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQmW2YsQYm9e","executionInfo":{"status":"ok","timestamp":1646055591523,"user_tz":-60,"elapsed":216,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"cbcc9f8c-108c-40b2-d9a7-cb21e3b7cd5e"},"source":["test_acc,attn_output_weights = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.2310\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYf2s3rA1tap","executionInfo":{"status":"ok","timestamp":1638728711164,"user_tz":-60,"elapsed":187,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"92ecf75d-1ff5-44a7-edde-55428444e98e"},"source":["attn_output_weights[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n","        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n","        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n","        ...,\n","        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n","        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n","        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"1JhVZCF32tZD"},"source":["Self-attention to all nodes with the first 1500 nodes to train, accuracy 23.10%.\n"]},{"cell_type":"markdown","metadata":{"id":"XCJysJ75W-XN"},"source":["**Applying self-attention with an attention mask (only self-loops) using the first 1500 nodes to train**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_OhPUrkv3keg","executionInfo":{"status":"ok","timestamp":1646055794685,"user_tz":-60,"elapsed":197,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"e794e709-e40c-4c34-a79d-d1d8ec8f1ed6"},"source":["#Creating an attention mask with all ones but 0 in the diagonal\n","att_mask=torch.ones(3327,3327)\n","att_mask.fill_diagonal_(0)\n","att_mask=att_mask.bool()\n","att_mask"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[False,  True,  True,  ...,  True,  True,  True],\n","        [ True, False,  True,  ...,  True,  True,  True],\n","        [ True,  True, False,  ...,  True,  True,  True],\n","        ...,\n","        [ True,  True,  True,  ..., False,  True,  True],\n","        [ True,  True,  True,  ...,  True, False,  True],\n","        [ True,  True,  True,  ...,  True,  True, False]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5WKlQ0gR6PTs","executionInfo":{"status":"ok","timestamp":1646055796824,"user_tz":-60,"elapsed":204,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"e6c88a6f-8561-4977-97dd-b2cfc693f836"},"source":["#Convert the input features dimension to the model features\n","#Apply self-attention using PyTorch MultiheadAttention\n","#Transform the encoder output to the output dimension\n","#Set up the model and tensors in cuda\n","import torch.nn\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","\n","class Self_Att(torch.nn.Module):\n","    def __init__(self):\n","        super(Self_Att, self).__init__()\n","        torch.manual_seed(12345)\n","        self.lin1 = Linear(3703,120)\n","        self.attention1 = torch.nn.MultiheadAttention(120,6)\n","        self.lin2 = Linear(120, dataset.num_classes)\n","\n","    def forward(self,x,att_mask):\n","        x = self.lin1(x)\n","        x = F.dropout(x, p=0.8, training=self.training)\n","        x, attn_output_weights = self.attention1(x,x,x,attn_mask=att_mask)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.lin2(x)\n","        x=torch.squeeze(x)#Remove the dimension of size 1\n","        return x,attn_output_weights\n","\n","\n","use_cuda = True\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","\n","model = Self_Att()\n","model.cuda()\n","\n","data=data.cuda()\n","data1=data1.cuda()\n","att_mask=att_mask.cuda()"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","metadata":{"id":"iJYuIRl76eIP","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1646055818361,"user_tz":-60,"elapsed":13742,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"8cb90610-13fe-4993-b435-9d5d89ce2d73"},"source":["#For training, the cross entropy loss combines LogSoftmax and NLLLoss in one single class\n","#input is expected to contain raw, unnormalized scores for each class\n","#target a class index in the range for each value of a 1D tensor of size minibatch\n","#For testing, compares the class with highest probability with the labels and counts the correct predictions\n","from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out,attn_output_weights = model(data1,att_mask)  # Perform a single forward pass.\n","      loss = criterion(out[0:1500], data.y[0:1500])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss,attn_output_weights\n","\n","def test():\n","      model.eval()\n","      out,attn_output_weights = model(data1,att_mask)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc,attn_output_weights\n","\n","for epoch in range(1, 201):\n","    loss,attn_output_weights = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n","    #print(attn_output_weights)"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 1.7803\n","Epoch: 002, Loss: 1.7700\n","Epoch: 003, Loss: 1.7512\n","Epoch: 004, Loss: 1.7445\n","Epoch: 005, Loss: 1.7364\n","Epoch: 006, Loss: 1.7306\n","Epoch: 007, Loss: 1.7262\n","Epoch: 008, Loss: 1.7153\n","Epoch: 009, Loss: 1.6874\n","Epoch: 010, Loss: 1.6451\n","Epoch: 011, Loss: 1.5767\n","Epoch: 012, Loss: 1.4859\n","Epoch: 013, Loss: 1.3884\n","Epoch: 014, Loss: 1.3093\n","Epoch: 015, Loss: 1.2276\n","Epoch: 016, Loss: 1.1319\n","Epoch: 017, Loss: 1.0653\n","Epoch: 018, Loss: 0.9622\n","Epoch: 019, Loss: 0.8921\n","Epoch: 020, Loss: 0.8197\n","Epoch: 021, Loss: 0.7359\n","Epoch: 022, Loss: 0.6766\n","Epoch: 023, Loss: 0.5958\n","Epoch: 024, Loss: 0.5371\n","Epoch: 025, Loss: 0.4690\n","Epoch: 026, Loss: 0.4280\n","Epoch: 027, Loss: 0.3744\n","Epoch: 028, Loss: 0.3375\n","Epoch: 029, Loss: 0.2867\n","Epoch: 030, Loss: 0.2415\n","Epoch: 031, Loss: 0.2159\n","Epoch: 032, Loss: 0.1864\n","Epoch: 033, Loss: 0.1777\n","Epoch: 034, Loss: 0.1403\n","Epoch: 035, Loss: 0.1457\n","Epoch: 036, Loss: 0.1427\n","Epoch: 037, Loss: 0.1161\n","Epoch: 038, Loss: 0.1082\n","Epoch: 039, Loss: 0.0917\n","Epoch: 040, Loss: 0.0990\n","Epoch: 041, Loss: 0.1133\n","Epoch: 042, Loss: 0.0990\n","Epoch: 043, Loss: 0.1212\n","Epoch: 044, Loss: 0.1152\n","Epoch: 045, Loss: 0.1014\n","Epoch: 046, Loss: 0.1099\n","Epoch: 047, Loss: 0.1108\n","Epoch: 048, Loss: 0.1137\n","Epoch: 049, Loss: 0.1195\n","Epoch: 050, Loss: 0.1198\n","Epoch: 051, Loss: 0.1166\n","Epoch: 052, Loss: 0.1093\n","Epoch: 053, Loss: 0.1144\n","Epoch: 054, Loss: 0.1252\n","Epoch: 055, Loss: 0.1148\n","Epoch: 056, Loss: 0.1149\n","Epoch: 057, Loss: 0.1245\n","Epoch: 058, Loss: 0.1162\n","Epoch: 059, Loss: 0.1215\n","Epoch: 060, Loss: 0.1029\n","Epoch: 061, Loss: 0.1036\n","Epoch: 062, Loss: 0.1162\n","Epoch: 063, Loss: 0.1055\n","Epoch: 064, Loss: 0.1109\n","Epoch: 065, Loss: 0.1087\n","Epoch: 066, Loss: 0.1093\n","Epoch: 067, Loss: 0.0931\n","Epoch: 068, Loss: 0.1275\n","Epoch: 069, Loss: 0.1061\n","Epoch: 070, Loss: 0.1067\n","Epoch: 071, Loss: 0.1043\n","Epoch: 072, Loss: 0.0938\n","Epoch: 073, Loss: 0.0980\n","Epoch: 074, Loss: 0.0967\n","Epoch: 075, Loss: 0.0840\n","Epoch: 076, Loss: 0.0806\n","Epoch: 077, Loss: 0.0871\n","Epoch: 078, Loss: 0.0973\n","Epoch: 079, Loss: 0.0925\n","Epoch: 080, Loss: 0.0906\n","Epoch: 081, Loss: 0.0845\n","Epoch: 082, Loss: 0.0695\n","Epoch: 083, Loss: 0.0930\n","Epoch: 084, Loss: 0.0717\n","Epoch: 085, Loss: 0.0633\n","Epoch: 086, Loss: 0.0707\n","Epoch: 087, Loss: 0.0777\n","Epoch: 088, Loss: 0.0752\n","Epoch: 089, Loss: 0.0777\n","Epoch: 090, Loss: 0.0692\n","Epoch: 091, Loss: 0.0752\n","Epoch: 092, Loss: 0.0637\n","Epoch: 093, Loss: 0.0662\n","Epoch: 094, Loss: 0.0617\n","Epoch: 095, Loss: 0.0673\n","Epoch: 096, Loss: 0.0593\n","Epoch: 097, Loss: 0.0629\n","Epoch: 098, Loss: 0.0713\n","Epoch: 099, Loss: 0.0683\n","Epoch: 100, Loss: 0.0740\n","Epoch: 101, Loss: 0.0668\n","Epoch: 102, Loss: 0.0538\n","Epoch: 103, Loss: 0.0600\n","Epoch: 104, Loss: 0.0675\n","Epoch: 105, Loss: 0.0650\n","Epoch: 106, Loss: 0.0613\n","Epoch: 107, Loss: 0.0575\n","Epoch: 108, Loss: 0.0761\n","Epoch: 109, Loss: 0.0526\n","Epoch: 110, Loss: 0.0642\n","Epoch: 111, Loss: 0.0656\n","Epoch: 112, Loss: 0.0640\n","Epoch: 113, Loss: 0.0582\n","Epoch: 114, Loss: 0.0895\n","Epoch: 115, Loss: 0.0707\n","Epoch: 116, Loss: 0.0730\n","Epoch: 117, Loss: 0.0631\n","Epoch: 118, Loss: 0.0780\n","Epoch: 119, Loss: 0.0630\n","Epoch: 120, Loss: 0.0700\n","Epoch: 121, Loss: 0.0685\n","Epoch: 122, Loss: 0.0717\n","Epoch: 123, Loss: 0.0813\n","Epoch: 124, Loss: 0.0634\n","Epoch: 125, Loss: 0.0636\n","Epoch: 126, Loss: 0.0642\n","Epoch: 127, Loss: 0.0810\n","Epoch: 128, Loss: 0.0833\n","Epoch: 129, Loss: 0.0576\n","Epoch: 130, Loss: 0.0707\n","Epoch: 131, Loss: 0.0767\n","Epoch: 132, Loss: 0.0809\n","Epoch: 133, Loss: 0.0920\n","Epoch: 134, Loss: 0.0755\n","Epoch: 135, Loss: 0.0672\n","Epoch: 136, Loss: 0.0609\n","Epoch: 137, Loss: 0.0704\n","Epoch: 138, Loss: 0.0693\n","Epoch: 139, Loss: 0.0612\n","Epoch: 140, Loss: 0.0746\n","Epoch: 141, Loss: 0.0765\n","Epoch: 142, Loss: 0.0783\n","Epoch: 143, Loss: 0.0827\n","Epoch: 144, Loss: 0.0805\n","Epoch: 145, Loss: 0.0719\n","Epoch: 146, Loss: 0.0767\n","Epoch: 147, Loss: 0.0659\n","Epoch: 148, Loss: 0.0830\n","Epoch: 149, Loss: 0.0839\n","Epoch: 150, Loss: 0.0859\n","Epoch: 151, Loss: 0.0716\n","Epoch: 152, Loss: 0.0718\n","Epoch: 153, Loss: 0.0689\n","Epoch: 154, Loss: 0.0772\n","Epoch: 155, Loss: 0.0713\n","Epoch: 156, Loss: 0.0744\n","Epoch: 157, Loss: 0.0774\n","Epoch: 158, Loss: 0.0684\n","Epoch: 159, Loss: 0.0705\n","Epoch: 160, Loss: 0.0869\n","Epoch: 161, Loss: 0.0871\n","Epoch: 162, Loss: 0.0791\n","Epoch: 163, Loss: 0.0742\n","Epoch: 164, Loss: 0.0884\n","Epoch: 165, Loss: 0.0725\n","Epoch: 166, Loss: 0.0866\n","Epoch: 167, Loss: 0.0745\n","Epoch: 168, Loss: 0.0693\n","Epoch: 169, Loss: 0.0817\n","Epoch: 170, Loss: 0.0919\n","Epoch: 171, Loss: 0.0848\n","Epoch: 172, Loss: 0.0847\n","Epoch: 173, Loss: 0.0716\n","Epoch: 174, Loss: 0.0915\n","Epoch: 175, Loss: 0.0661\n","Epoch: 176, Loss: 0.0802\n","Epoch: 177, Loss: 0.0785\n","Epoch: 178, Loss: 0.0940\n","Epoch: 179, Loss: 0.0788\n","Epoch: 180, Loss: 0.0581\n","Epoch: 181, Loss: 0.0706\n","Epoch: 182, Loss: 0.0656\n","Epoch: 183, Loss: 0.0790\n","Epoch: 184, Loss: 0.0920\n","Epoch: 185, Loss: 0.0917\n","Epoch: 186, Loss: 0.0710\n","Epoch: 187, Loss: 0.0685\n","Epoch: 188, Loss: 0.0815\n","Epoch: 189, Loss: 0.0835\n","Epoch: 190, Loss: 0.0772\n","Epoch: 191, Loss: 0.0815\n","Epoch: 192, Loss: 0.0742\n","Epoch: 193, Loss: 0.0743\n","Epoch: 194, Loss: 0.0689\n","Epoch: 195, Loss: 0.0654\n","Epoch: 196, Loss: 0.0781\n","Epoch: 197, Loss: 0.0642\n","Epoch: 198, Loss: 0.0644\n","Epoch: 199, Loss: 0.0640\n","Epoch: 200, Loss: 0.0825\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Zx1hKDl__Mi","executionInfo":{"status":"ok","timestamp":1646055820452,"user_tz":-60,"elapsed":180,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"49765fd8-cbde-4f85-9be4-be3eeccef93d"},"source":["test_acc,attn_output_weights = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.6910\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8frCO6iN4B9n","executionInfo":{"status":"ok","timestamp":1646055824258,"user_tz":-60,"elapsed":188,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"5feabd11-b03f-4e42-d9de-5068b3f30aaf"},"source":["attn_output_weights"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 1., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 1.,  ..., 0., 0., 0.],\n","         ...,\n","         [0., 0., 0.,  ..., 1., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 1., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 1.]]], device='cuda:0',\n","       grad_fn=<DivBackward0>)"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"g_KkeERhEYzv"},"source":["Self-attention to only the same node with the first 1500 nodes to train, accuracy 69.10%."]},{"cell_type":"markdown","metadata":{"id":"2Gf9nYXUHUvP"},"source":["**Using Transformers with an attention mask (representing the graph structure) using the first 1500 nodes to train**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U6h2sjUoHT79","executionInfo":{"status":"ok","timestamp":1646055889001,"user_tz":-60,"elapsed":1493,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"1d033604-e684-4f68-d74b-8083a7673bfa"},"source":["import numpy as np\n","import networkx as nx\n","from torch_geometric.utils import to_networkx\n","\n","G = to_networkx(data, to_undirected=False)\n","att_mask=nx.to_numpy_matrix(G)\n","np.fill_diagonal(att_mask,1)\n","\n","zero_indices = att_mask == 0\n","non_zero_indices = att_mask != 0\n","att_mask[non_zero_indices] = 0\n","att_mask[zero_indices] = 1\n","\n","att_mask=torch.from_numpy(att_mask)\n","att_mask=att_mask.bool()\n","att_mask"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[False,  True,  True,  ...,  True,  True,  True],\n","        [ True, False,  True,  ...,  True,  True,  True],\n","        [ True,  True, False,  ...,  True,  True,  True],\n","        ...,\n","        [ True,  True,  True,  ..., False,  True,  True],\n","        [ True,  True,  True,  ...,  True, False,  True],\n","        [ True,  True,  True,  ...,  True,  True, False]])"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sfd56hKQKgFS","executionInfo":{"status":"ok","timestamp":1646055891774,"user_tz":-60,"elapsed":182,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"63d126b4-a397-4943-b296-3f764cf53622"},"source":["#Convert the input features dimension to the model features\n","#Apply self-attention using PyTorch MultiheadAttention\n","#Transform the encoder output to the output dimension\n","#Set up the model and tensors in cuda\n","import torch.nn\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","\n","class Self_Att(torch.nn.Module):\n","    def __init__(self):\n","        super(Self_Att, self).__init__()\n","        torch.manual_seed(12345)\n","        self.lin1 = Linear(3703,120)\n","        self.attention1 = torch.nn.MultiheadAttention(120,6)\n","        self.lin2 = Linear(120, dataset.num_classes)\n","\n","    def forward(self,x,att_mask):\n","        x = self.lin1(x)\n","        x = F.dropout(x, p=0.8, training=self.training)\n","        x, attn_output_weights = self.attention1(x,x,x,attn_mask=att_mask)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.lin2(x)\n","        x=torch.squeeze(x)#Remove the dimension of size 1\n","        return x,attn_output_weights\n","\n","\n","use_cuda = True\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","\n","model = Self_Att()\n","model.cuda()\n","\n","data=data.cuda()\n","data1=data1.cuda()\n","att_mask=att_mask.cuda()"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","metadata":{"id":"KHbximhcKvWH","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1646055914930,"user_tz":-60,"elapsed":13835,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"10e3ff0b-9404-4139-f133-4d70c4049788"},"source":["#For training, the cross entropy loss combines LogSoftmax and NLLLoss in one single class\n","#input is expected to contain raw, unnormalized scores for each class\n","#target a class index in the range for each value of a 1D tensor of size minibatch\n","#For testing, compares the class with highest probability with the labels and counts the correct predictions\n","from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out,attn_output_weights = model(data1,att_mask)  # Perform a single forward pass.\n","      loss = criterion(out[0:1500], data.y[0:1500])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss,attn_output_weights\n","\n","def test():\n","      model.eval()\n","      out,attn_output_weights = model(data1,att_mask)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc,attn_output_weights\n","\n","for epoch in range(1, 201):\n","    loss,attn_output_weights = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n","    #print(attn_output_weights)"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 1.7802\n","Epoch: 002, Loss: 1.7703\n","Epoch: 003, Loss: 1.7507\n","Epoch: 004, Loss: 1.7453\n","Epoch: 005, Loss: 1.7379\n","Epoch: 006, Loss: 1.7326\n","Epoch: 007, Loss: 1.7294\n","Epoch: 008, Loss: 1.7234\n","Epoch: 009, Loss: 1.7043\n","Epoch: 010, Loss: 1.6766\n","Epoch: 011, Loss: 1.6234\n","Epoch: 012, Loss: 1.5507\n","Epoch: 013, Loss: 1.4606\n","Epoch: 014, Loss: 1.3573\n","Epoch: 015, Loss: 1.2662\n","Epoch: 016, Loss: 1.1814\n","Epoch: 017, Loss: 1.0849\n","Epoch: 018, Loss: 1.0176\n","Epoch: 019, Loss: 0.9325\n","Epoch: 020, Loss: 0.8770\n","Epoch: 021, Loss: 0.8124\n","Epoch: 022, Loss: 0.7600\n","Epoch: 023, Loss: 0.7068\n","Epoch: 024, Loss: 0.6577\n","Epoch: 025, Loss: 0.5970\n","Epoch: 026, Loss: 0.5467\n","Epoch: 027, Loss: 0.5210\n","Epoch: 028, Loss: 0.4806\n","Epoch: 029, Loss: 0.4479\n","Epoch: 030, Loss: 0.4468\n","Epoch: 031, Loss: 0.4443\n","Epoch: 032, Loss: 0.3636\n","Epoch: 033, Loss: 0.3701\n","Epoch: 034, Loss: 0.3505\n","Epoch: 035, Loss: 0.3562\n","Epoch: 036, Loss: 0.2985\n","Epoch: 037, Loss: 0.3411\n","Epoch: 038, Loss: 0.2853\n","Epoch: 039, Loss: 0.2715\n","Epoch: 040, Loss: 0.2810\n","Epoch: 041, Loss: 0.2649\n","Epoch: 042, Loss: 0.2594\n","Epoch: 043, Loss: 0.2232\n","Epoch: 044, Loss: 0.2454\n","Epoch: 045, Loss: 0.2272\n","Epoch: 046, Loss: 0.2274\n","Epoch: 047, Loss: 0.2150\n","Epoch: 048, Loss: 0.2265\n","Epoch: 049, Loss: 0.2172\n","Epoch: 050, Loss: 0.2174\n","Epoch: 051, Loss: 0.2579\n","Epoch: 052, Loss: 0.2394\n","Epoch: 053, Loss: 0.2221\n","Epoch: 054, Loss: 0.2340\n","Epoch: 055, Loss: 0.1781\n","Epoch: 056, Loss: 0.2159\n","Epoch: 057, Loss: 0.1742\n","Epoch: 058, Loss: 0.1945\n","Epoch: 059, Loss: 0.1817\n","Epoch: 060, Loss: 0.1767\n","Epoch: 061, Loss: 0.1613\n","Epoch: 062, Loss: 0.1828\n","Epoch: 063, Loss: 0.1666\n","Epoch: 064, Loss: 0.2017\n","Epoch: 065, Loss: 0.1663\n","Epoch: 066, Loss: 0.1764\n","Epoch: 067, Loss: 0.1836\n","Epoch: 068, Loss: 0.1673\n","Epoch: 069, Loss: 0.1684\n","Epoch: 070, Loss: 0.1774\n","Epoch: 071, Loss: 0.1694\n","Epoch: 072, Loss: 0.1747\n","Epoch: 073, Loss: 0.1773\n","Epoch: 074, Loss: 0.1676\n","Epoch: 075, Loss: 0.1621\n","Epoch: 076, Loss: 0.1598\n","Epoch: 077, Loss: 0.1704\n","Epoch: 078, Loss: 0.1444\n","Epoch: 079, Loss: 0.1860\n","Epoch: 080, Loss: 0.1604\n","Epoch: 081, Loss: 0.1668\n","Epoch: 082, Loss: 0.1672\n","Epoch: 083, Loss: 0.1613\n","Epoch: 084, Loss: 0.1637\n","Epoch: 085, Loss: 0.1687\n","Epoch: 086, Loss: 0.1516\n","Epoch: 087, Loss: 0.1709\n","Epoch: 088, Loss: 0.1507\n","Epoch: 089, Loss: 0.1577\n","Epoch: 090, Loss: 0.1448\n","Epoch: 091, Loss: 0.1490\n","Epoch: 092, Loss: 0.1586\n","Epoch: 093, Loss: 0.1864\n","Epoch: 094, Loss: 0.1582\n","Epoch: 095, Loss: 0.1794\n","Epoch: 096, Loss: 0.1440\n","Epoch: 097, Loss: 0.1699\n","Epoch: 098, Loss: 0.1482\n","Epoch: 099, Loss: 0.1999\n","Epoch: 100, Loss: 0.1597\n","Epoch: 101, Loss: 0.1725\n","Epoch: 102, Loss: 0.1680\n","Epoch: 103, Loss: 0.1744\n","Epoch: 104, Loss: 0.1537\n","Epoch: 105, Loss: 0.1760\n","Epoch: 106, Loss: 0.1687\n","Epoch: 107, Loss: 0.1667\n","Epoch: 108, Loss: 0.1770\n","Epoch: 109, Loss: 0.1559\n","Epoch: 110, Loss: 0.1658\n","Epoch: 111, Loss: 0.1555\n","Epoch: 112, Loss: 0.1545\n","Epoch: 113, Loss: 0.1561\n","Epoch: 114, Loss: 0.1659\n","Epoch: 115, Loss: 0.1744\n","Epoch: 116, Loss: 0.1430\n","Epoch: 117, Loss: 0.1698\n","Epoch: 118, Loss: 0.1512\n","Epoch: 119, Loss: 0.1460\n","Epoch: 120, Loss: 0.1568\n","Epoch: 121, Loss: 0.1552\n","Epoch: 122, Loss: 0.1475\n","Epoch: 123, Loss: 0.1409\n","Epoch: 124, Loss: 0.1403\n","Epoch: 125, Loss: 0.1514\n","Epoch: 126, Loss: 0.1528\n","Epoch: 127, Loss: 0.1497\n","Epoch: 128, Loss: 0.1322\n","Epoch: 129, Loss: 0.1421\n","Epoch: 130, Loss: 0.1460\n","Epoch: 131, Loss: 0.1461\n","Epoch: 132, Loss: 0.1488\n","Epoch: 133, Loss: 0.1491\n","Epoch: 134, Loss: 0.1502\n","Epoch: 135, Loss: 0.1523\n","Epoch: 136, Loss: 0.1428\n","Epoch: 137, Loss: 0.1513\n","Epoch: 138, Loss: 0.1554\n","Epoch: 139, Loss: 0.1433\n","Epoch: 140, Loss: 0.1434\n","Epoch: 141, Loss: 0.1449\n","Epoch: 142, Loss: 0.1463\n","Epoch: 143, Loss: 0.1422\n","Epoch: 144, Loss: 0.1587\n","Epoch: 145, Loss: 0.1410\n","Epoch: 146, Loss: 0.1333\n","Epoch: 147, Loss: 0.1387\n","Epoch: 148, Loss: 0.1341\n","Epoch: 149, Loss: 0.1572\n","Epoch: 150, Loss: 0.1324\n","Epoch: 151, Loss: 0.1498\n","Epoch: 152, Loss: 0.1526\n","Epoch: 153, Loss: 0.1617\n","Epoch: 154, Loss: 0.1505\n","Epoch: 155, Loss: 0.1361\n","Epoch: 156, Loss: 0.1462\n","Epoch: 157, Loss: 0.1630\n","Epoch: 158, Loss: 0.1619\n","Epoch: 159, Loss: 0.1403\n","Epoch: 160, Loss: 0.1341\n","Epoch: 161, Loss: 0.1344\n","Epoch: 162, Loss: 0.1485\n","Epoch: 163, Loss: 0.1708\n","Epoch: 164, Loss: 0.1652\n","Epoch: 165, Loss: 0.1588\n","Epoch: 166, Loss: 0.1607\n","Epoch: 167, Loss: 0.1463\n","Epoch: 168, Loss: 0.1777\n","Epoch: 169, Loss: 0.1278\n","Epoch: 170, Loss: 0.1575\n","Epoch: 171, Loss: 0.1408\n","Epoch: 172, Loss: 0.1489\n","Epoch: 173, Loss: 0.1588\n","Epoch: 174, Loss: 0.1461\n","Epoch: 175, Loss: 0.1476\n","Epoch: 176, Loss: 0.1691\n","Epoch: 177, Loss: 0.1428\n","Epoch: 178, Loss: 0.1717\n","Epoch: 179, Loss: 0.1648\n","Epoch: 180, Loss: 0.1556\n","Epoch: 181, Loss: 0.1701\n","Epoch: 182, Loss: 0.1480\n","Epoch: 183, Loss: 0.1636\n","Epoch: 184, Loss: 0.1420\n","Epoch: 185, Loss: 0.1566\n","Epoch: 186, Loss: 0.1278\n","Epoch: 187, Loss: 0.1518\n","Epoch: 188, Loss: 0.1335\n","Epoch: 189, Loss: 0.1517\n","Epoch: 190, Loss: 0.1527\n","Epoch: 191, Loss: 0.1492\n","Epoch: 192, Loss: 0.1479\n","Epoch: 193, Loss: 0.1486\n","Epoch: 194, Loss: 0.1237\n","Epoch: 195, Loss: 0.1841\n","Epoch: 196, Loss: 0.1684\n","Epoch: 197, Loss: 0.1724\n","Epoch: 198, Loss: 0.1726\n","Epoch: 199, Loss: 0.1511\n","Epoch: 200, Loss: 0.1491\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"64C_wzE8L33c","executionInfo":{"status":"ok","timestamp":1646055917175,"user_tz":-60,"elapsed":183,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"7ab170c3-a720-49fe-ef7e-14c05161309a"},"source":["test_acc,attn_output_weights = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7300\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dz64QTXV54Gs","executionInfo":{"status":"ok","timestamp":1646055919373,"user_tz":-60,"elapsed":185,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"04f84b65-58cf-468b-9d44-7f820a135cd0"},"source":["data.edge_index[:,0:10]"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[   0,    1,    1,    1,    1,    1,    2,    3,    3,    4],\n","        [ 628,  158,  486, 1097, 2919, 2933, 3285, 1431, 3219,  467]],\n","       device='cuda:0')"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["print(attn_output_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M0IcENxH0UkX","executionInfo":{"status":"ok","timestamp":1646055921403,"user_tz":-60,"elapsed":185,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"3cf85f1f-ff35-4242-9960-2f12cba56304"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.5000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.1667, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.5000,  ..., 0.0000, 0.0000, 0.0000],\n","         ...,\n","         [0.0000, 0.0000, 0.0000,  ..., 0.2500, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.5000]]],\n","       device='cuda:0', grad_fn=<DivBackward0>)\n"]}]},{"cell_type":"code","source":["def model_summary(model):\n","    \n","    model_params_list = list(model.named_parameters())\n","    print(\"----------------------------------------------------------------\")\n","    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer.Parameter\", \"Param Tensor Shape\", \"Param #\")\n","    print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    for elem in model_params_list:\n","        p_name = elem[0] \n","        p_shape = list(elem[1].size())\n","        p_count = torch.tensor(elem[1].size()).prod().item()\n","        line_new = \"{:>20}  {:>25} {:>15}\".format(p_name, str(p_shape), str(p_count))\n","        print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    total_params = sum([param.nelement() for param in model.parameters()])\n","    print(\"Total params:\", total_params)\n","    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"Trainable params:\", num_trainable_params)\n","    print(\"Non-trainable params:\", total_params - num_trainable_params)\n","\n","model_summary(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bFNai9u1e41","executionInfo":{"status":"ok","timestamp":1646055927598,"user_tz":-60,"elapsed":178,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"ebf59140-b8ba-47b7-d9de-8a244a0b7ea9"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","     Layer.Parameter         Param Tensor Shape         Param #\n","----------------------------------------------------------------\n","         lin1.weight                [120, 3703]          444360\n","           lin1.bias                      [120]             120\n","attention1.in_proj_weight                 [360, 120]           43200\n","attention1.in_proj_bias                      [360]             360\n","attention1.out_proj.weight                 [120, 120]           14400\n","attention1.out_proj.bias                      [120]             120\n","         lin2.weight                   [6, 120]             720\n","           lin2.bias                        [6]               6\n","----------------------------------------------------------------\n","Total params: 503286\n","Trainable params: 503286\n","Non-trainable params: 0\n"]}]},{"cell_type":"markdown","source":["Self-attention to the neighboring nodes with the first 1500 nodes to train, accuracy 73%."],"metadata":{"id":"W0Hn5fJsrI4A"}},{"cell_type":"markdown","metadata":{"id":"XWSVkTn_FfGF"},"source":["**The basic model but in Cuda for comparisions**"]},{"cell_type":"code","metadata":{"id":"VCYTZ1eqEbhO"},"source":["import torch\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","\n","\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self, hidden_channels):\n","        super(GCN, self).__init__()\n","        torch.manual_seed(1234567)\n","        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","        x = self.conv1(x, edge_index)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return x\n","\n","model = GCN(hidden_channels=16)\n","\n","model.cuda()\n","data=data.cuda()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"lR1KVfvVEiZq","executionInfo":{"status":"ok","timestamp":1639047405255,"user_tz":-60,"elapsed":1413,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"f5df2912-8b9e-4646-879c-ba723f195fde"},"source":["from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out = model(data.x,data.edge_index)  # Perform a single forward pass.\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss\n","\n","def test():\n","      model.eval()\n","      out = model(data.x,data.edge_index)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc\n","\n","\n","for epoch in range(1, 201):\n","    loss = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 1.7915\n","Epoch: 002, Loss: 1.7875\n","Epoch: 003, Loss: 1.7823\n","Epoch: 004, Loss: 1.7752\n","Epoch: 005, Loss: 1.7683\n","Epoch: 006, Loss: 1.7605\n","Epoch: 007, Loss: 1.7525\n","Epoch: 008, Loss: 1.7423\n","Epoch: 009, Loss: 1.7387\n","Epoch: 010, Loss: 1.7282\n","Epoch: 011, Loss: 1.7168\n","Epoch: 012, Loss: 1.7133\n","Epoch: 013, Loss: 1.6932\n","Epoch: 014, Loss: 1.6868\n","Epoch: 015, Loss: 1.6759\n","Epoch: 016, Loss: 1.6629\n","Epoch: 017, Loss: 1.6535\n","Epoch: 018, Loss: 1.6446\n","Epoch: 019, Loss: 1.6305\n","Epoch: 020, Loss: 1.6317\n","Epoch: 021, Loss: 1.6121\n","Epoch: 022, Loss: 1.6036\n","Epoch: 023, Loss: 1.5931\n","Epoch: 024, Loss: 1.5798\n","Epoch: 025, Loss: 1.5575\n","Epoch: 026, Loss: 1.5395\n","Epoch: 027, Loss: 1.5365\n","Epoch: 028, Loss: 1.5098\n","Epoch: 029, Loss: 1.4833\n","Epoch: 030, Loss: 1.4816\n","Epoch: 031, Loss: 1.4661\n","Epoch: 032, Loss: 1.4568\n","Epoch: 033, Loss: 1.4259\n","Epoch: 034, Loss: 1.4174\n","Epoch: 035, Loss: 1.4302\n","Epoch: 036, Loss: 1.4050\n","Epoch: 037, Loss: 1.3624\n","Epoch: 038, Loss: 1.3600\n","Epoch: 039, Loss: 1.3464\n","Epoch: 040, Loss: 1.2972\n","Epoch: 041, Loss: 1.2932\n","Epoch: 042, Loss: 1.2640\n","Epoch: 043, Loss: 1.3010\n","Epoch: 044, Loss: 1.2461\n","Epoch: 045, Loss: 1.2579\n","Epoch: 046, Loss: 1.2150\n","Epoch: 047, Loss: 1.1948\n","Epoch: 048, Loss: 1.1877\n","Epoch: 049, Loss: 1.1802\n","Epoch: 050, Loss: 1.1640\n","Epoch: 051, Loss: 1.1240\n","Epoch: 052, Loss: 1.1319\n","Epoch: 053, Loss: 1.1034\n","Epoch: 054, Loss: 1.0958\n","Epoch: 055, Loss: 1.0653\n","Epoch: 056, Loss: 1.0779\n","Epoch: 057, Loss: 1.1009\n","Epoch: 058, Loss: 1.0206\n","Epoch: 059, Loss: 1.0315\n","Epoch: 060, Loss: 1.0150\n","Epoch: 061, Loss: 0.9945\n","Epoch: 062, Loss: 1.0052\n","Epoch: 063, Loss: 0.9563\n","Epoch: 064, Loss: 0.9470\n","Epoch: 065, Loss: 0.9453\n","Epoch: 066, Loss: 0.9658\n","Epoch: 067, Loss: 0.8888\n","Epoch: 068, Loss: 0.9505\n","Epoch: 069, Loss: 0.8900\n","Epoch: 070, Loss: 0.8760\n","Epoch: 071, Loss: 0.8373\n","Epoch: 072, Loss: 0.8740\n","Epoch: 073, Loss: 0.8816\n","Epoch: 074, Loss: 0.8558\n","Epoch: 075, Loss: 0.8019\n","Epoch: 076, Loss: 0.8260\n","Epoch: 077, Loss: 0.8505\n","Epoch: 078, Loss: 0.8272\n","Epoch: 079, Loss: 0.8141\n","Epoch: 080, Loss: 0.7889\n","Epoch: 081, Loss: 0.7788\n","Epoch: 082, Loss: 0.7280\n","Epoch: 083, Loss: 0.7378\n","Epoch: 084, Loss: 0.7622\n","Epoch: 085, Loss: 0.7702\n","Epoch: 086, Loss: 0.8006\n","Epoch: 087, Loss: 0.7354\n","Epoch: 088, Loss: 0.7113\n","Epoch: 089, Loss: 0.7172\n","Epoch: 090, Loss: 0.6920\n","Epoch: 091, Loss: 0.7217\n","Epoch: 092, Loss: 0.6746\n","Epoch: 093, Loss: 0.7152\n","Epoch: 094, Loss: 0.7302\n","Epoch: 095, Loss: 0.6796\n","Epoch: 096, Loss: 0.6606\n","Epoch: 097, Loss: 0.6642\n","Epoch: 098, Loss: 0.6642\n","Epoch: 099, Loss: 0.6851\n","Epoch: 100, Loss: 0.6636\n","Epoch: 101, Loss: 0.6177\n","Epoch: 102, Loss: 0.6245\n","Epoch: 103, Loss: 0.6095\n","Epoch: 104, Loss: 0.6181\n","Epoch: 105, Loss: 0.6079\n","Epoch: 106, Loss: 0.6221\n","Epoch: 107, Loss: 0.6271\n","Epoch: 108, Loss: 0.6452\n","Epoch: 109, Loss: 0.6114\n","Epoch: 110, Loss: 0.6071\n","Epoch: 111, Loss: 0.5959\n","Epoch: 112, Loss: 0.5926\n","Epoch: 113, Loss: 0.6016\n","Epoch: 114, Loss: 0.5942\n","Epoch: 115, Loss: 0.5663\n","Epoch: 116, Loss: 0.5433\n","Epoch: 117, Loss: 0.5747\n","Epoch: 118, Loss: 0.6145\n","Epoch: 119, Loss: 0.5651\n","Epoch: 120, Loss: 0.6027\n","Epoch: 121, Loss: 0.5679\n","Epoch: 122, Loss: 0.5412\n","Epoch: 123, Loss: 0.5613\n","Epoch: 124, Loss: 0.5629\n","Epoch: 125, Loss: 0.5262\n","Epoch: 126, Loss: 0.5450\n","Epoch: 127, Loss: 0.5472\n","Epoch: 128, Loss: 0.5443\n","Epoch: 129, Loss: 0.5515\n","Epoch: 130, Loss: 0.5662\n","Epoch: 131, Loss: 0.5156\n","Epoch: 132, Loss: 0.5297\n","Epoch: 133, Loss: 0.5539\n","Epoch: 134, Loss: 0.5504\n","Epoch: 135, Loss: 0.5485\n","Epoch: 136, Loss: 0.5345\n","Epoch: 137, Loss: 0.4880\n","Epoch: 138, Loss: 0.4787\n","Epoch: 139, Loss: 0.5319\n","Epoch: 140, Loss: 0.5130\n","Epoch: 141, Loss: 0.4945\n","Epoch: 142, Loss: 0.4686\n","Epoch: 143, Loss: 0.5053\n","Epoch: 144, Loss: 0.4832\n","Epoch: 145, Loss: 0.4842\n","Epoch: 146, Loss: 0.5055\n","Epoch: 147, Loss: 0.4849\n","Epoch: 148, Loss: 0.4947\n","Epoch: 149, Loss: 0.5094\n","Epoch: 150, Loss: 0.4725\n","Epoch: 151, Loss: 0.4761\n","Epoch: 152, Loss: 0.4698\n","Epoch: 153, Loss: 0.4658\n","Epoch: 154, Loss: 0.4809\n","Epoch: 155, Loss: 0.4642\n","Epoch: 156, Loss: 0.4690\n","Epoch: 157, Loss: 0.4769\n","Epoch: 158, Loss: 0.4554\n","Epoch: 159, Loss: 0.4907\n","Epoch: 160, Loss: 0.5107\n","Epoch: 161, Loss: 0.4553\n","Epoch: 162, Loss: 0.4410\n","Epoch: 163, Loss: 0.4547\n","Epoch: 164, Loss: 0.4309\n","Epoch: 165, Loss: 0.4221\n","Epoch: 166, Loss: 0.4606\n","Epoch: 167, Loss: 0.4778\n","Epoch: 168, Loss: 0.4821\n","Epoch: 169, Loss: 0.4155\n","Epoch: 170, Loss: 0.4278\n","Epoch: 171, Loss: 0.4190\n","Epoch: 172, Loss: 0.4528\n","Epoch: 173, Loss: 0.4219\n","Epoch: 174, Loss: 0.4290\n","Epoch: 175, Loss: 0.4412\n","Epoch: 176, Loss: 0.4818\n","Epoch: 177, Loss: 0.4446\n","Epoch: 178, Loss: 0.4397\n","Epoch: 179, Loss: 0.4413\n","Epoch: 180, Loss: 0.4390\n","Epoch: 181, Loss: 0.4516\n","Epoch: 182, Loss: 0.4324\n","Epoch: 183, Loss: 0.4009\n","Epoch: 184, Loss: 0.4110\n","Epoch: 185, Loss: 0.4230\n","Epoch: 186, Loss: 0.4189\n","Epoch: 187, Loss: 0.3886\n","Epoch: 188, Loss: 0.4305\n","Epoch: 189, Loss: 0.4454\n","Epoch: 190, Loss: 0.4351\n","Epoch: 191, Loss: 0.4292\n","Epoch: 192, Loss: 0.4393\n","Epoch: 193, Loss: 0.3926\n","Epoch: 194, Loss: 0.4831\n","Epoch: 195, Loss: 0.4343\n","Epoch: 196, Loss: 0.3775\n","Epoch: 197, Loss: 0.4321\n","Epoch: 198, Loss: 0.3740\n","Epoch: 199, Loss: 0.4082\n","Epoch: 200, Loss: 0.4241\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iwKHMjNYFJHH","executionInfo":{"status":"ok","timestamp":1639047409093,"user_tz":-60,"elapsed":352,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"97b2db74-ea5f-492a-a3ba-c9243d32d2a7"},"source":["test_acc = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7090\n"]}]},{"cell_type":"code","source":["def model_summary(model):\n","    \n","    model_params_list = list(model.named_parameters())\n","    print(\"----------------------------------------------------------------\")\n","    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer.Parameter\", \"Param Tensor Shape\", \"Param #\")\n","    print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    for elem in model_params_list:\n","        p_name = elem[0] \n","        p_shape = list(elem[1].size())\n","        p_count = torch.tensor(elem[1].size()).prod().item()\n","        line_new = \"{:>20}  {:>25} {:>15}\".format(p_name, str(p_shape), str(p_count))\n","        print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    total_params = sum([param.nelement() for param in model.parameters()])\n","    print(\"Total params:\", total_params)\n","    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"Trainable params:\", num_trainable_params)\n","    print(\"Non-trainable params:\", total_params - num_trainable_params)\n","\n","model_summary(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xF0HfNmp1mY3","executionInfo":{"status":"ok","timestamp":1639047412355,"user_tz":-60,"elapsed":10,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"8831cfcd-d6b4-4cd3-9778-c5defa48673c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","     Layer.Parameter         Param Tensor Shape         Param #\n","----------------------------------------------------------------\n","          conv1.bias                       [16]              16\n","    conv1.lin.weight                 [16, 3703]           59248\n","          conv2.bias                        [6]               6\n","    conv2.lin.weight                    [6, 16]              96\n","----------------------------------------------------------------\n","Total params: 59366\n","Trainable params: 59366\n","Non-trainable params: 0\n"]}]}]}