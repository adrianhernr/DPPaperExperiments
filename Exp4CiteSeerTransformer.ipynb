{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exp4CiteSeerTransformer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOj+knGuwdZF/ElYXcONSPx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fYvxMt6P-g8e","executionInfo":{"status":"ok","timestamp":1645977946982,"user_tz":-60,"elapsed":11356,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"78c1f254-c060-4df8-f556-7d1fae221a16"},"source":["#Check the PyTorch and Cuda version\n","!python -c \"import torch; print(torch.__version__)\"\n","!python -c \"import torch; print(torch.version.cuda)\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n","11.1\n"]}]},{"cell_type":"code","metadata":{"id":"ICFCAiNp-qHq"},"source":["!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LgzQTkyL-4GJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646054097004,"user_tz":-60,"elapsed":9410,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"584c1e57-0be8-4fbf-b721-9fa4d9d8ad56"},"source":["#First data characteristics\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.transforms import NormalizeFeatures\n","\n","dataset = Planetoid(root='/tmp/CiteSeer', name='CiteSeer',transform=NormalizeFeatures())\n","\n","data = dataset[0]\n","print(data)\n","\n","print(f'Number of nodes: {data.num_nodes}')\n","print(f'Nodes features: {data.num_node_features}')\n","print(f'Number of classes: {dataset.num_classes}')\n","print(f'Number of edges: {data.num_edges}')\n","print(f'Avarage degree: {data.num_edges / data.num_nodes:.2f}')\n","print(f'Training nodes: {data.train_mask.sum()}')\n","print(f'Validation nodes: {data.val_mask.sum()}')\n","print(f'Test nodes: {data.test_mask.sum()}')\n","print(f'Isolated nodes: {data.has_isolated_nodes()}')\n","print(f'Loops: {data.has_self_loops()}')\n","print(f'Is undirected: {data.is_undirected()}')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n","Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n","Processing...\n"]},{"output_type":"stream","name":"stdout","text":["Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])\n","Number of nodes: 3327\n","Nodes features: 3703\n","Number of classes: 6\n","Number of edges: 9104\n","Avarage degree: 2.74\n","Training nodes: 120\n","Validation nodes: 500\n","Test nodes: 1000\n","Isolated nodes: True\n","Loops: False\n","Is undirected: True\n"]},{"output_type":"stream","name":"stderr","text":["Done!\n"]}]},{"cell_type":"markdown","metadata":{"id":"98tFjZzDyAF8"},"source":["**Applying self-attention to all nodes**\n","\n","Using GNNs, the prediction of each node is done using neural network layers on an aggregation of neighboring nodes. Now we are going to use the building block of a Transformer, applying self-attention to all nodes of the graph. A part of these nodes will be used for training and another for validation and test."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMvHVxzkRc4d","executionInfo":{"status":"ok","timestamp":1646054119662,"user_tz":-60,"elapsed":189,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"df330df7-d65b-4004-b808-c63c70516e2d"},"source":["#Add a new batch dimension of size 1 to the tensor\n","import torch\n","\n","data1=torch.unsqueeze(data.x, 1)\n","data1.shape"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3327, 1, 3703])"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"Kzomc_siIzt3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646054298465,"user_tz":-60,"elapsed":9732,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"054a9eb7-f5b5-4925-95b0-38ced787f0b1"},"source":["#Convert the input features dimension to the model features\n","#Apply self-attention using PyTorch MultiheadAttention\n","#Transform the encoder output to the output dimension\n","#Set up the model and tensors in cuda\n","import torch.nn\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","\n","class Self_Att(torch.nn.Module):\n","    def __init__(self):\n","        super(Self_Att, self).__init__()\n","        torch.manual_seed(12345)\n","        self.lin1 = Linear(3703,120)\n","        self.attention1 = torch.nn.MultiheadAttention(120,6)\n","        self.lin2 = Linear(120, dataset.num_classes)\n","\n","    def forward(self,x):\n","        x = self.lin1(x)\n","        x = F.dropout(x, p=0.8, training=self.training)\n","        x, attn_output_weights = self.attention1(x,x,x)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.lin2(x)\n","        x=torch.squeeze(x)#Remove the dimension of size 1\n","        return x,attn_output_weights\n","\n","use_cuda = True\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","\n","model = Self_Att()\n","model.cuda()\n","\n","data=data.cuda()\n","data1=data1.cuda()\n","\n","print(model)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Self_Att(\n","  (lin1): Linear(in_features=3703, out_features=120, bias=True)\n","  (attention1): MultiheadAttention(\n","    (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n","  )\n","  (lin2): Linear(in_features=120, out_features=6, bias=True)\n",")\n"]}]},{"cell_type":"code","metadata":{"id":"RwsWAdtMVjej","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1646054351716,"user_tz":-60,"elapsed":12909,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"3246a3d3-151e-4782-e49f-edb13ec67a0a"},"source":["#For training, the cross entropy loss combines LogSoftmax and NLLLoss in one single class\n","#input is expected to contain raw, unnormalized scores for each class\n","#target a class index in the range for each value of a 1D tensor of size minibatch\n","#For testing, compares the class with highest probability with the labels and counts the correct predictions\n","from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out,attn_output_weights = model(data1)  # Perform a single forward pass.\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss,attn_output_weights\n","\n","def test():\n","      model.eval()\n","      out,attn_output_weights = model(data1)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc,attn_output_weights\n","\n","for epoch in range(1, 201):\n","    loss,attn_output_weights = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n","    #print(attn_output_weights)"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 1.7938\n","Epoch: 002, Loss: 1.7929\n","Epoch: 003, Loss: 1.7911\n","Epoch: 004, Loss: 1.7897\n","Epoch: 005, Loss: 1.7937\n","Epoch: 006, Loss: 1.7922\n","Epoch: 007, Loss: 1.7896\n","Epoch: 008, Loss: 1.7902\n","Epoch: 009, Loss: 1.7926\n","Epoch: 010, Loss: 1.7886\n","Epoch: 011, Loss: 1.7932\n","Epoch: 012, Loss: 1.7869\n","Epoch: 013, Loss: 1.7948\n","Epoch: 014, Loss: 1.7935\n","Epoch: 015, Loss: 1.7851\n","Epoch: 016, Loss: 1.7965\n","Epoch: 017, Loss: 1.7934\n","Epoch: 018, Loss: 1.7928\n","Epoch: 019, Loss: 1.7899\n","Epoch: 020, Loss: 1.7893\n","Epoch: 021, Loss: 1.7888\n","Epoch: 022, Loss: 1.7948\n","Epoch: 023, Loss: 1.7890\n","Epoch: 024, Loss: 1.7910\n","Epoch: 025, Loss: 1.8000\n","Epoch: 026, Loss: 1.7942\n","Epoch: 027, Loss: 1.7910\n","Epoch: 028, Loss: 1.7902\n","Epoch: 029, Loss: 1.7930\n","Epoch: 030, Loss: 1.7956\n","Epoch: 031, Loss: 1.7911\n","Epoch: 032, Loss: 1.7916\n","Epoch: 033, Loss: 1.7913\n","Epoch: 034, Loss: 1.7943\n","Epoch: 035, Loss: 1.7922\n","Epoch: 036, Loss: 1.7918\n","Epoch: 037, Loss: 1.7906\n","Epoch: 038, Loss: 1.7917\n","Epoch: 039, Loss: 1.7938\n","Epoch: 040, Loss: 1.7932\n","Epoch: 041, Loss: 1.7929\n","Epoch: 042, Loss: 1.7903\n","Epoch: 043, Loss: 1.7893\n","Epoch: 044, Loss: 1.7936\n","Epoch: 045, Loss: 1.7905\n","Epoch: 046, Loss: 1.7930\n","Epoch: 047, Loss: 1.7926\n","Epoch: 048, Loss: 1.7908\n","Epoch: 049, Loss: 1.7913\n","Epoch: 050, Loss: 1.7911\n","Epoch: 051, Loss: 1.7914\n","Epoch: 052, Loss: 1.7914\n","Epoch: 053, Loss: 1.7919\n","Epoch: 054, Loss: 1.7937\n","Epoch: 055, Loss: 1.7942\n","Epoch: 056, Loss: 1.7885\n","Epoch: 057, Loss: 1.7941\n","Epoch: 058, Loss: 1.7900\n","Epoch: 059, Loss: 1.7915\n","Epoch: 060, Loss: 1.7915\n","Epoch: 061, Loss: 1.7947\n","Epoch: 062, Loss: 1.7921\n","Epoch: 063, Loss: 1.7905\n","Epoch: 064, Loss: 1.7928\n","Epoch: 065, Loss: 1.7923\n","Epoch: 066, Loss: 1.7894\n","Epoch: 067, Loss: 1.7925\n","Epoch: 068, Loss: 1.7901\n","Epoch: 069, Loss: 1.7933\n","Epoch: 070, Loss: 1.7962\n","Epoch: 071, Loss: 1.7922\n","Epoch: 072, Loss: 1.7958\n","Epoch: 073, Loss: 1.7910\n","Epoch: 074, Loss: 1.7912\n","Epoch: 075, Loss: 1.7910\n","Epoch: 076, Loss: 1.7919\n","Epoch: 077, Loss: 1.7885\n","Epoch: 078, Loss: 1.7919\n","Epoch: 079, Loss: 1.7896\n","Epoch: 080, Loss: 1.7910\n","Epoch: 081, Loss: 1.7908\n","Epoch: 082, Loss: 1.7934\n","Epoch: 083, Loss: 1.7924\n","Epoch: 084, Loss: 1.7848\n","Epoch: 085, Loss: 1.7929\n","Epoch: 086, Loss: 1.7899\n","Epoch: 087, Loss: 1.7960\n","Epoch: 088, Loss: 1.7910\n","Epoch: 089, Loss: 1.7866\n","Epoch: 090, Loss: 1.7870\n","Epoch: 091, Loss: 1.8024\n","Epoch: 092, Loss: 1.7967\n","Epoch: 093, Loss: 1.7919\n","Epoch: 094, Loss: 1.7907\n","Epoch: 095, Loss: 1.7906\n","Epoch: 096, Loss: 1.7921\n","Epoch: 097, Loss: 1.7901\n","Epoch: 098, Loss: 1.7898\n","Epoch: 099, Loss: 1.7897\n","Epoch: 100, Loss: 1.7948\n","Epoch: 101, Loss: 1.7978\n","Epoch: 102, Loss: 1.7928\n","Epoch: 103, Loss: 1.7903\n","Epoch: 104, Loss: 1.7916\n","Epoch: 105, Loss: 1.7959\n","Epoch: 106, Loss: 1.7945\n","Epoch: 107, Loss: 1.7985\n","Epoch: 108, Loss: 1.7931\n","Epoch: 109, Loss: 1.7902\n","Epoch: 110, Loss: 1.7928\n","Epoch: 111, Loss: 1.7944\n","Epoch: 112, Loss: 1.7925\n","Epoch: 113, Loss: 1.7945\n","Epoch: 114, Loss: 1.7926\n","Epoch: 115, Loss: 1.7926\n","Epoch: 116, Loss: 1.7934\n","Epoch: 117, Loss: 1.7912\n","Epoch: 118, Loss: 1.7929\n","Epoch: 119, Loss: 1.7907\n","Epoch: 120, Loss: 1.7929\n","Epoch: 121, Loss: 1.7937\n","Epoch: 122, Loss: 1.7927\n","Epoch: 123, Loss: 1.7921\n","Epoch: 124, Loss: 1.7926\n","Epoch: 125, Loss: 1.7909\n","Epoch: 126, Loss: 1.7918\n","Epoch: 127, Loss: 1.7909\n","Epoch: 128, Loss: 1.7899\n","Epoch: 129, Loss: 1.7921\n","Epoch: 130, Loss: 1.7938\n","Epoch: 131, Loss: 1.7922\n","Epoch: 132, Loss: 1.7930\n","Epoch: 133, Loss: 1.7936\n","Epoch: 134, Loss: 1.7916\n","Epoch: 135, Loss: 1.7907\n","Epoch: 136, Loss: 1.7936\n","Epoch: 137, Loss: 1.7920\n","Epoch: 138, Loss: 1.7925\n","Epoch: 139, Loss: 1.7897\n","Epoch: 140, Loss: 1.7939\n","Epoch: 141, Loss: 1.7918\n","Epoch: 142, Loss: 1.7929\n","Epoch: 143, Loss: 1.7911\n","Epoch: 144, Loss: 1.7916\n","Epoch: 145, Loss: 1.7911\n","Epoch: 146, Loss: 1.7908\n","Epoch: 147, Loss: 1.7909\n","Epoch: 148, Loss: 1.7944\n","Epoch: 149, Loss: 1.7922\n","Epoch: 150, Loss: 1.7919\n","Epoch: 151, Loss: 1.7896\n","Epoch: 152, Loss: 1.7931\n","Epoch: 153, Loss: 1.7904\n","Epoch: 154, Loss: 1.7905\n","Epoch: 155, Loss: 1.7948\n","Epoch: 156, Loss: 1.7905\n","Epoch: 157, Loss: 1.7926\n","Epoch: 158, Loss: 1.7919\n","Epoch: 159, Loss: 1.7901\n","Epoch: 160, Loss: 1.7899\n","Epoch: 161, Loss: 1.7928\n","Epoch: 162, Loss: 1.7915\n","Epoch: 163, Loss: 1.7914\n","Epoch: 164, Loss: 1.7915\n","Epoch: 165, Loss: 1.7923\n","Epoch: 166, Loss: 1.7914\n","Epoch: 167, Loss: 1.7914\n","Epoch: 168, Loss: 1.7888\n","Epoch: 169, Loss: 1.7927\n","Epoch: 170, Loss: 1.7886\n","Epoch: 171, Loss: 1.7924\n","Epoch: 172, Loss: 1.7931\n","Epoch: 173, Loss: 1.7936\n","Epoch: 174, Loss: 1.7953\n","Epoch: 175, Loss: 1.7928\n","Epoch: 176, Loss: 1.7903\n","Epoch: 177, Loss: 1.7903\n","Epoch: 178, Loss: 1.7901\n","Epoch: 179, Loss: 1.7928\n","Epoch: 180, Loss: 1.7937\n","Epoch: 181, Loss: 1.7913\n","Epoch: 182, Loss: 1.7907\n","Epoch: 183, Loss: 1.7945\n","Epoch: 184, Loss: 1.7917\n","Epoch: 185, Loss: 1.7939\n","Epoch: 186, Loss: 1.7966\n","Epoch: 187, Loss: 1.7932\n","Epoch: 188, Loss: 1.7920\n","Epoch: 189, Loss: 1.7908\n","Epoch: 190, Loss: 1.7919\n","Epoch: 191, Loss: 1.7924\n","Epoch: 192, Loss: 1.7926\n","Epoch: 193, Loss: 1.7896\n","Epoch: 194, Loss: 1.7900\n","Epoch: 195, Loss: 1.7923\n","Epoch: 196, Loss: 1.7912\n","Epoch: 197, Loss: 1.7911\n","Epoch: 198, Loss: 1.7885\n","Epoch: 199, Loss: 1.7946\n","Epoch: 200, Loss: 1.7992\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQmW2YsQYm9e","executionInfo":{"status":"ok","timestamp":1646054370019,"user_tz":-60,"elapsed":221,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"4069547d-49c6-4009-ce38-483d5bac0144"},"source":["test_acc,attn_output_weights = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.1810\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYf2s3rA1tap","executionInfo":{"status":"ok","timestamp":1638728711164,"user_tz":-60,"elapsed":187,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"92ecf75d-1ff5-44a7-edde-55428444e98e"},"source":["attn_output_weights[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n","        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n","        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n","        ...,\n","        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n","        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n","        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003]],\n","       device='cuda:0', grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"1JhVZCF32tZD"},"source":["Self-attention to all nodes, accuracy 18.10% (training loss 1.799).\n"]},{"cell_type":"markdown","metadata":{"id":"XCJysJ75W-XN"},"source":["**Applying self-attention with an attention mask (only self-loops)**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_OhPUrkv3keg","executionInfo":{"status":"ok","timestamp":1646054422655,"user_tz":-60,"elapsed":215,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"3441249c-2c1d-48db-9d5b-f81dc8e2a5f6"},"source":["#Creating an attention mask with all ones but 0 in the diagonal\n","att_mask=torch.ones(3327,3327)\n","att_mask.fill_diagonal_(0)\n","att_mask=att_mask.bool()\n","att_mask"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[False,  True,  True,  ...,  True,  True,  True],\n","        [ True, False,  True,  ...,  True,  True,  True],\n","        [ True,  True, False,  ...,  True,  True,  True],\n","        ...,\n","        [ True,  True,  True,  ..., False,  True,  True],\n","        [ True,  True,  True,  ...,  True, False,  True],\n","        [ True,  True,  True,  ...,  True,  True, False]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5WKlQ0gR6PTs","executionInfo":{"status":"ok","timestamp":1646054451948,"user_tz":-60,"elapsed":196,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"c2f811a1-2357-4893-fffc-a66735c2ec70"},"source":["#Convert the input features dimension to the model features\n","#Apply self-attention using PyTorch MultiheadAttention\n","#Transform the encoder output to the output dimension\n","#Set up the model and tensors in cuda\n","import torch.nn\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","\n","class Self_Att(torch.nn.Module):\n","    def __init__(self):\n","        super(Self_Att, self).__init__()\n","        torch.manual_seed(12345)\n","        self.lin1 = Linear(3703,120)\n","        self.attention1 = torch.nn.MultiheadAttention(120,6)\n","        self.lin2 = Linear(120, dataset.num_classes)\n","\n","    def forward(self,x,att_mask):\n","        x = self.lin1(x)\n","        x = F.dropout(x, p=0.8, training=self.training)\n","        x, attn_output_weights = self.attention1(x,x,x,attn_mask=att_mask)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.lin2(x)\n","        x=torch.squeeze(x)#Remove the dimension of size 1\n","        return x,attn_output_weights\n","\n","\n","use_cuda = True\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","\n","model = Self_Att()\n","model.cuda()\n","\n","data=data.cuda()\n","data1=data1.cuda()\n","att_mask=att_mask.cuda()"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","metadata":{"id":"iJYuIRl76eIP","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1646054475834,"user_tz":-60,"elapsed":14335,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"66018c7b-df80-41be-b6f6-27f74d880c34"},"source":["#For training, the cross entropy loss combines LogSoftmax and NLLLoss in one single class\n","#input is expected to contain raw, unnormalized scores for each class\n","#target a class index in the range for each value of a 1D tensor of size minibatch\n","#For testing, compares the class with highest probability with the labels and counts the correct predictions\n","from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out,attn_output_weights = model(data1,att_mask)  # Perform a single forward pass.\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss,attn_output_weights\n","\n","def test():\n","      model.eval()\n","      out,attn_output_weights = model(data1,att_mask)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc,attn_output_weights\n","\n","for epoch in range(1, 201):\n","    loss,attn_output_weights = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n","    #print(attn_output_weights)"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 1.7941\n","Epoch: 002, Loss: 1.7916\n","Epoch: 003, Loss: 1.7879\n","Epoch: 004, Loss: 1.7784\n","Epoch: 005, Loss: 1.7702\n","Epoch: 006, Loss: 1.7506\n","Epoch: 007, Loss: 1.7392\n","Epoch: 008, Loss: 1.6970\n","Epoch: 009, Loss: 1.6334\n","Epoch: 010, Loss: 1.5111\n","Epoch: 011, Loss: 1.3679\n","Epoch: 012, Loss: 1.2415\n","Epoch: 013, Loss: 1.0849\n","Epoch: 014, Loss: 0.8486\n","Epoch: 015, Loss: 0.7535\n","Epoch: 016, Loss: 0.5429\n","Epoch: 017, Loss: 0.4522\n","Epoch: 018, Loss: 0.2763\n","Epoch: 019, Loss: 0.2294\n","Epoch: 020, Loss: 0.1670\n","Epoch: 021, Loss: 0.1116\n","Epoch: 022, Loss: 0.0832\n","Epoch: 023, Loss: 0.0541\n","Epoch: 024, Loss: 0.0988\n","Epoch: 025, Loss: 0.0376\n","Epoch: 026, Loss: 0.0165\n","Epoch: 027, Loss: 0.0216\n","Epoch: 028, Loss: 0.0305\n","Epoch: 029, Loss: 0.0351\n","Epoch: 030, Loss: 0.0577\n","Epoch: 031, Loss: 0.0118\n","Epoch: 032, Loss: 0.0228\n","Epoch: 033, Loss: 0.0342\n","Epoch: 034, Loss: 0.0363\n","Epoch: 035, Loss: 0.1472\n","Epoch: 036, Loss: 0.0116\n","Epoch: 037, Loss: 0.1232\n","Epoch: 038, Loss: 0.0240\n","Epoch: 039, Loss: 0.1376\n","Epoch: 040, Loss: 0.1174\n","Epoch: 041, Loss: 0.0839\n","Epoch: 042, Loss: 0.0167\n","Epoch: 043, Loss: 0.0353\n","Epoch: 044, Loss: 0.1268\n","Epoch: 045, Loss: 0.1055\n","Epoch: 046, Loss: 0.0837\n","Epoch: 047, Loss: 0.0534\n","Epoch: 048, Loss: 0.0755\n","Epoch: 049, Loss: 0.1428\n","Epoch: 050, Loss: 0.1995\n","Epoch: 051, Loss: 0.1233\n","Epoch: 052, Loss: 0.0519\n","Epoch: 053, Loss: 0.1935\n","Epoch: 054, Loss: 0.1532\n","Epoch: 055, Loss: 0.1245\n","Epoch: 056, Loss: 0.1391\n","Epoch: 057, Loss: 0.1205\n","Epoch: 058, Loss: 0.0362\n","Epoch: 059, Loss: 0.0916\n","Epoch: 060, Loss: 0.0935\n","Epoch: 061, Loss: 0.0337\n","Epoch: 062, Loss: 0.1045\n","Epoch: 063, Loss: 0.0652\n","Epoch: 064, Loss: 0.2006\n","Epoch: 065, Loss: 0.0728\n","Epoch: 066, Loss: 0.1073\n","Epoch: 067, Loss: 0.0297\n","Epoch: 068, Loss: 0.0789\n","Epoch: 069, Loss: 0.1950\n","Epoch: 070, Loss: 0.0731\n","Epoch: 071, Loss: 0.0222\n","Epoch: 072, Loss: 0.0971\n","Epoch: 073, Loss: 0.0323\n","Epoch: 074, Loss: 0.1150\n","Epoch: 075, Loss: 0.0705\n","Epoch: 076, Loss: 0.1206\n","Epoch: 077, Loss: 0.1701\n","Epoch: 078, Loss: 0.0360\n","Epoch: 079, Loss: 0.0883\n","Epoch: 080, Loss: 0.0849\n","Epoch: 081, Loss: 0.1190\n","Epoch: 082, Loss: 0.1405\n","Epoch: 083, Loss: 0.0858\n","Epoch: 084, Loss: 0.0764\n","Epoch: 085, Loss: 0.0570\n","Epoch: 086, Loss: 0.2426\n","Epoch: 087, Loss: 0.0682\n","Epoch: 088, Loss: 0.0513\n","Epoch: 089, Loss: 0.0450\n","Epoch: 090, Loss: 0.0920\n","Epoch: 091, Loss: 0.0641\n","Epoch: 092, Loss: 0.0432\n","Epoch: 093, Loss: 0.1842\n","Epoch: 094, Loss: 0.0573\n","Epoch: 095, Loss: 0.0937\n","Epoch: 096, Loss: 0.0512\n","Epoch: 097, Loss: 0.1456\n","Epoch: 098, Loss: 0.0341\n","Epoch: 099, Loss: 0.0287\n","Epoch: 100, Loss: 0.0404\n","Epoch: 101, Loss: 0.0293\n","Epoch: 102, Loss: 0.0932\n","Epoch: 103, Loss: 0.1113\n","Epoch: 104, Loss: 0.1039\n","Epoch: 105, Loss: 0.0264\n","Epoch: 106, Loss: 0.0677\n","Epoch: 107, Loss: 0.0498\n","Epoch: 108, Loss: 0.0548\n","Epoch: 109, Loss: 0.0772\n","Epoch: 110, Loss: 0.0307\n","Epoch: 111, Loss: 0.0229\n","Epoch: 112, Loss: 0.0234\n","Epoch: 113, Loss: 0.0218\n","Epoch: 114, Loss: 0.0537\n","Epoch: 115, Loss: 0.0374\n","Epoch: 116, Loss: 0.0264\n","Epoch: 117, Loss: 0.1108\n","Epoch: 118, Loss: 0.0407\n","Epoch: 119, Loss: 0.0088\n","Epoch: 120, Loss: 0.0277\n","Epoch: 121, Loss: 0.1120\n","Epoch: 122, Loss: 0.0428\n","Epoch: 123, Loss: 0.0409\n","Epoch: 124, Loss: 0.0523\n","Epoch: 125, Loss: 0.0605\n","Epoch: 126, Loss: 0.0376\n","Epoch: 127, Loss: 0.0265\n","Epoch: 128, Loss: 0.0504\n","Epoch: 129, Loss: 0.0220\n","Epoch: 130, Loss: 0.0173\n","Epoch: 131, Loss: 0.0368\n","Epoch: 132, Loss: 0.0492\n","Epoch: 133, Loss: 0.0275\n","Epoch: 134, Loss: 0.0183\n","Epoch: 135, Loss: 0.0134\n","Epoch: 136, Loss: 0.0144\n","Epoch: 137, Loss: 0.0470\n","Epoch: 138, Loss: 0.0874\n","Epoch: 139, Loss: 0.0157\n","Epoch: 140, Loss: 0.0267\n","Epoch: 141, Loss: 0.0237\n","Epoch: 142, Loss: 0.0294\n","Epoch: 143, Loss: 0.0480\n","Epoch: 144, Loss: 0.0255\n","Epoch: 145, Loss: 0.0188\n","Epoch: 146, Loss: 0.0398\n","Epoch: 147, Loss: 0.0113\n","Epoch: 148, Loss: 0.0112\n","Epoch: 149, Loss: 0.0399\n","Epoch: 150, Loss: 0.0374\n","Epoch: 151, Loss: 0.0168\n","Epoch: 152, Loss: 0.0089\n","Epoch: 153, Loss: 0.0236\n","Epoch: 154, Loss: 0.0093\n","Epoch: 155, Loss: 0.0202\n","Epoch: 156, Loss: 0.0247\n","Epoch: 157, Loss: 0.0236\n","Epoch: 158, Loss: 0.0454\n","Epoch: 159, Loss: 0.0330\n","Epoch: 160, Loss: 0.0107\n","Epoch: 161, Loss: 0.0452\n","Epoch: 162, Loss: 0.0219\n","Epoch: 163, Loss: 0.0107\n","Epoch: 164, Loss: 0.0155\n","Epoch: 165, Loss: 0.0363\n","Epoch: 166, Loss: 0.0307\n","Epoch: 167, Loss: 0.0224\n","Epoch: 168, Loss: 0.0182\n","Epoch: 169, Loss: 0.0318\n","Epoch: 170, Loss: 0.0196\n","Epoch: 171, Loss: 0.0213\n","Epoch: 172, Loss: 0.0210\n","Epoch: 173, Loss: 0.0112\n","Epoch: 174, Loss: 0.0189\n","Epoch: 175, Loss: 0.0441\n","Epoch: 176, Loss: 0.0139\n","Epoch: 177, Loss: 0.0170\n","Epoch: 178, Loss: 0.0323\n","Epoch: 179, Loss: 0.0067\n","Epoch: 180, Loss: 0.0207\n","Epoch: 181, Loss: 0.0109\n","Epoch: 182, Loss: 0.0232\n","Epoch: 183, Loss: 0.0135\n","Epoch: 184, Loss: 0.0123\n","Epoch: 185, Loss: 0.0180\n","Epoch: 186, Loss: 0.0165\n","Epoch: 187, Loss: 0.0239\n","Epoch: 188, Loss: 0.0082\n","Epoch: 189, Loss: 0.0166\n","Epoch: 190, Loss: 0.0171\n","Epoch: 191, Loss: 0.0193\n","Epoch: 192, Loss: 0.0117\n","Epoch: 193, Loss: 0.0233\n","Epoch: 194, Loss: 0.0252\n","Epoch: 195, Loss: 0.0111\n","Epoch: 196, Loss: 0.0443\n","Epoch: 197, Loss: 0.0156\n","Epoch: 198, Loss: 0.0260\n","Epoch: 199, Loss: 0.0363\n","Epoch: 200, Loss: 0.0114\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Zx1hKDl__Mi","executionInfo":{"status":"ok","timestamp":1646054479568,"user_tz":-60,"elapsed":259,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"f760b6f8-f9ac-49e3-d9ae-9d521c06c422"},"source":["test_acc,attn_output_weights = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.5130\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8frCO6iN4B9n","executionInfo":{"status":"ok","timestamp":1646054482837,"user_tz":-60,"elapsed":196,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"f105c72e-a935-4798-9374-9d28b833363f"},"source":["attn_output_weights"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n","         [0., 1., 0.,  ..., 0., 0., 0.],\n","         [0., 0., 1.,  ..., 0., 0., 0.],\n","         ...,\n","         [0., 0., 0.,  ..., 1., 0., 0.],\n","         [0., 0., 0.,  ..., 0., 1., 0.],\n","         [0., 0., 0.,  ..., 0., 0., 1.]]], device='cuda:0',\n","       grad_fn=<DivBackward0>)"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"g_KkeERhEYzv"},"source":["Self-attention to only the same node, accuracy 51.30% (training loss 0.01/0.02/0.03)."]},{"cell_type":"markdown","metadata":{"id":"2Gf9nYXUHUvP"},"source":["**Using Transformers with an attention mask (representing the graph structure)**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U6h2sjUoHT79","executionInfo":{"status":"ok","timestamp":1646054542791,"user_tz":-60,"elapsed":1723,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"cd07a644-03cc-48cd-e38f-6139aee18635"},"source":["import numpy as np\n","import networkx as nx\n","from torch_geometric.utils import to_networkx\n","\n","G = to_networkx(data, to_undirected=False)\n","att_mask=nx.to_numpy_matrix(G)\n","np.fill_diagonal(att_mask,1)\n","\n","zero_indices = att_mask == 0\n","non_zero_indices = att_mask != 0\n","att_mask[non_zero_indices] = 0\n","att_mask[zero_indices] = 1\n","\n","att_mask=torch.from_numpy(att_mask)\n","att_mask=att_mask.bool()\n","att_mask"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[False,  True,  True,  ...,  True,  True,  True],\n","        [ True, False,  True,  ...,  True,  True,  True],\n","        [ True,  True, False,  ...,  True,  True,  True],\n","        ...,\n","        [ True,  True,  True,  ..., False,  True,  True],\n","        [ True,  True,  True,  ...,  True, False,  True],\n","        [ True,  True,  True,  ...,  True,  True, False]])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sfd56hKQKgFS","executionInfo":{"status":"ok","timestamp":1646054548421,"user_tz":-60,"elapsed":206,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"55a4d69d-5451-4db6-dae3-f959aa67ebe4"},"source":["#Convert the input features dimension to the model features\n","#Apply self-attention using PyTorch MultiheadAttention\n","#Transform the encoder output to the output dimension\n","#Set up the model and tensors in cuda\n","import torch.nn\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","\n","class Self_Att(torch.nn.Module):\n","    def __init__(self):\n","        super(Self_Att, self).__init__()\n","        torch.manual_seed(12345)\n","        self.lin1 = Linear(3703,120)\n","        self.attention1 = torch.nn.MultiheadAttention(120,6)\n","        self.lin2 = Linear(120, dataset.num_classes)\n","\n","    def forward(self,x,att_mask):\n","        x = self.lin1(x)\n","        x = F.dropout(x, p=0.8, training=self.training)\n","        x, attn_output_weights = self.attention1(x,x,x,attn_mask=att_mask)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.2, training=self.training)\n","        x = self.lin2(x)\n","        x=torch.squeeze(x)#Remove the dimension of size 1\n","        return x,attn_output_weights\n","\n","\n","use_cuda = True\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","\n","model = Self_Att()\n","model.cuda()\n","\n","data=data.cuda()\n","data1=data1.cuda()\n","att_mask=att_mask.cuda()"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","metadata":{"id":"KHbximhcKvWH","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1646054575162,"user_tz":-60,"elapsed":13964,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"e6110e7d-2eb8-498d-f045-f5263e497706"},"source":["#For training, the cross entropy loss combines LogSoftmax and NLLLoss in one single class\n","#input is expected to contain raw, unnormalized scores for each class\n","#target a class index in the range for each value of a 1D tensor of size minibatch\n","#For testing, compares the class with highest probability with the labels and counts the correct predictions\n","from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out,attn_output_weights = model(data1,att_mask)  # Perform a single forward pass.\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss,attn_output_weights\n","\n","def test():\n","      model.eval()\n","      out,attn_output_weights = model(data1,att_mask)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc,attn_output_weights\n","\n","for epoch in range(1, 201):\n","    loss,attn_output_weights = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n","    #print(attn_output_weights)"],"execution_count":15,"outputs":[{"output_type":"display_data","data":{"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 1.7941\n","Epoch: 002, Loss: 1.7919\n","Epoch: 003, Loss: 1.7885\n","Epoch: 004, Loss: 1.7829\n","Epoch: 005, Loss: 1.7822\n","Epoch: 006, Loss: 1.7675\n","Epoch: 007, Loss: 1.7458\n","Epoch: 008, Loss: 1.7022\n","Epoch: 009, Loss: 1.6561\n","Epoch: 010, Loss: 1.5725\n","Epoch: 011, Loss: 1.4412\n","Epoch: 012, Loss: 1.2772\n","Epoch: 013, Loss: 1.0938\n","Epoch: 014, Loss: 0.9235\n","Epoch: 015, Loss: 0.7221\n","Epoch: 016, Loss: 0.5210\n","Epoch: 017, Loss: 0.3721\n","Epoch: 018, Loss: 0.2689\n","Epoch: 019, Loss: 0.2426\n","Epoch: 020, Loss: 0.1628\n","Epoch: 021, Loss: 0.0889\n","Epoch: 022, Loss: 0.0819\n","Epoch: 023, Loss: 0.0691\n","Epoch: 024, Loss: 0.0509\n","Epoch: 025, Loss: 0.0445\n","Epoch: 026, Loss: 0.0269\n","Epoch: 027, Loss: 0.0571\n","Epoch: 028, Loss: 0.0443\n","Epoch: 029, Loss: 0.0875\n","Epoch: 030, Loss: 0.0286\n","Epoch: 031, Loss: 0.0082\n","Epoch: 032, Loss: 0.0628\n","Epoch: 033, Loss: 0.0840\n","Epoch: 034, Loss: 0.0192\n","Epoch: 035, Loss: 0.1089\n","Epoch: 036, Loss: 0.0270\n","Epoch: 037, Loss: 0.0133\n","Epoch: 038, Loss: 0.0566\n","Epoch: 039, Loss: 0.0465\n","Epoch: 040, Loss: 0.0136\n","Epoch: 041, Loss: 0.0451\n","Epoch: 042, Loss: 0.0199\n","Epoch: 043, Loss: 0.0398\n","Epoch: 044, Loss: 0.0639\n","Epoch: 045, Loss: 0.0160\n","Epoch: 046, Loss: 0.0903\n","Epoch: 047, Loss: 0.0086\n","Epoch: 048, Loss: 0.0549\n","Epoch: 049, Loss: 0.0194\n","Epoch: 050, Loss: 0.0135\n","Epoch: 051, Loss: 0.0237\n","Epoch: 052, Loss: 0.0180\n","Epoch: 053, Loss: 0.0138\n","Epoch: 054, Loss: 0.0095\n","Epoch: 055, Loss: 0.0241\n","Epoch: 056, Loss: 0.0247\n","Epoch: 057, Loss: 0.0116\n","Epoch: 058, Loss: 0.0075\n","Epoch: 059, Loss: 0.0574\n","Epoch: 060, Loss: 0.0273\n","Epoch: 061, Loss: 0.0188\n","Epoch: 062, Loss: 0.0535\n","Epoch: 063, Loss: 0.0465\n","Epoch: 064, Loss: 0.0156\n","Epoch: 065, Loss: 0.0171\n","Epoch: 066, Loss: 0.0288\n","Epoch: 067, Loss: 0.0275\n","Epoch: 068, Loss: 0.0167\n","Epoch: 069, Loss: 0.0145\n","Epoch: 070, Loss: 0.0133\n","Epoch: 071, Loss: 0.0247\n","Epoch: 072, Loss: 0.0331\n","Epoch: 073, Loss: 0.0305\n","Epoch: 074, Loss: 0.0391\n","Epoch: 075, Loss: 0.0289\n","Epoch: 076, Loss: 0.0130\n","Epoch: 077, Loss: 0.0166\n","Epoch: 078, Loss: 0.0215\n","Epoch: 079, Loss: 0.0148\n","Epoch: 080, Loss: 0.0113\n","Epoch: 081, Loss: 0.0063\n","Epoch: 082, Loss: 0.0189\n","Epoch: 083, Loss: 0.0236\n","Epoch: 084, Loss: 0.0560\n","Epoch: 085, Loss: 0.0083\n","Epoch: 086, Loss: 0.0152\n","Epoch: 087, Loss: 0.0603\n","Epoch: 088, Loss: 0.0097\n","Epoch: 089, Loss: 0.0168\n","Epoch: 090, Loss: 0.0143\n","Epoch: 091, Loss: 0.0309\n","Epoch: 092, Loss: 0.0226\n","Epoch: 093, Loss: 0.0374\n","Epoch: 094, Loss: 0.0341\n","Epoch: 095, Loss: 0.0124\n","Epoch: 096, Loss: 0.0239\n","Epoch: 097, Loss: 0.0453\n","Epoch: 098, Loss: 0.0195\n","Epoch: 099, Loss: 0.0350\n","Epoch: 100, Loss: 0.0141\n","Epoch: 101, Loss: 0.0306\n","Epoch: 102, Loss: 0.0168\n","Epoch: 103, Loss: 0.0148\n","Epoch: 104, Loss: 0.0376\n","Epoch: 105, Loss: 0.0245\n","Epoch: 106, Loss: 0.0092\n","Epoch: 107, Loss: 0.0232\n","Epoch: 108, Loss: 0.0114\n","Epoch: 109, Loss: 0.0174\n","Epoch: 110, Loss: 0.0149\n","Epoch: 111, Loss: 0.0141\n","Epoch: 112, Loss: 0.0160\n","Epoch: 113, Loss: 0.0183\n","Epoch: 114, Loss: 0.0114\n","Epoch: 115, Loss: 0.0181\n","Epoch: 116, Loss: 0.0258\n","Epoch: 117, Loss: 0.0135\n","Epoch: 118, Loss: 0.0085\n","Epoch: 119, Loss: 0.0120\n","Epoch: 120, Loss: 0.0104\n","Epoch: 121, Loss: 0.0128\n","Epoch: 122, Loss: 0.0280\n","Epoch: 123, Loss: 0.0186\n","Epoch: 124, Loss: 0.0159\n","Epoch: 125, Loss: 0.0322\n","Epoch: 126, Loss: 0.0263\n","Epoch: 127, Loss: 0.0089\n","Epoch: 128, Loss: 0.0225\n","Epoch: 129, Loss: 0.0238\n","Epoch: 130, Loss: 0.0105\n","Epoch: 131, Loss: 0.0116\n","Epoch: 132, Loss: 0.0097\n","Epoch: 133, Loss: 0.0260\n","Epoch: 134, Loss: 0.0201\n","Epoch: 135, Loss: 0.0196\n","Epoch: 136, Loss: 0.0101\n","Epoch: 137, Loss: 0.0381\n","Epoch: 138, Loss: 0.0284\n","Epoch: 139, Loss: 0.0190\n","Epoch: 140, Loss: 0.0192\n","Epoch: 141, Loss: 0.0191\n","Epoch: 142, Loss: 0.0147\n","Epoch: 143, Loss: 0.0141\n","Epoch: 144, Loss: 0.0103\n","Epoch: 145, Loss: 0.0277\n","Epoch: 146, Loss: 0.0128\n","Epoch: 147, Loss: 0.0392\n","Epoch: 148, Loss: 0.0192\n","Epoch: 149, Loss: 0.0171\n","Epoch: 150, Loss: 0.0112\n","Epoch: 151, Loss: 0.0349\n","Epoch: 152, Loss: 0.0205\n","Epoch: 153, Loss: 0.0107\n","Epoch: 154, Loss: 0.0250\n","Epoch: 155, Loss: 0.0315\n","Epoch: 156, Loss: 0.0206\n","Epoch: 157, Loss: 0.0215\n","Epoch: 158, Loss: 0.0131\n","Epoch: 159, Loss: 0.0143\n","Epoch: 160, Loss: 0.0143\n","Epoch: 161, Loss: 0.0231\n","Epoch: 162, Loss: 0.0290\n","Epoch: 163, Loss: 0.0136\n","Epoch: 164, Loss: 0.0126\n","Epoch: 165, Loss: 0.0171\n","Epoch: 166, Loss: 0.0096\n","Epoch: 167, Loss: 0.0272\n","Epoch: 168, Loss: 0.0297\n","Epoch: 169, Loss: 0.0131\n","Epoch: 170, Loss: 0.0281\n","Epoch: 171, Loss: 0.0276\n","Epoch: 172, Loss: 0.0105\n","Epoch: 173, Loss: 0.0126\n","Epoch: 174, Loss: 0.0194\n","Epoch: 175, Loss: 0.0106\n","Epoch: 176, Loss: 0.0178\n","Epoch: 177, Loss: 0.0224\n","Epoch: 178, Loss: 0.0147\n","Epoch: 179, Loss: 0.0148\n","Epoch: 180, Loss: 0.0177\n","Epoch: 181, Loss: 0.0076\n","Epoch: 182, Loss: 0.0093\n","Epoch: 183, Loss: 0.0518\n","Epoch: 184, Loss: 0.0252\n","Epoch: 185, Loss: 0.0301\n","Epoch: 186, Loss: 0.0280\n","Epoch: 187, Loss: 0.0199\n","Epoch: 188, Loss: 0.0533\n","Epoch: 189, Loss: 0.0119\n","Epoch: 190, Loss: 0.0123\n","Epoch: 191, Loss: 0.0136\n","Epoch: 192, Loss: 0.0174\n","Epoch: 193, Loss: 0.0437\n","Epoch: 194, Loss: 0.0264\n","Epoch: 195, Loss: 0.0170\n","Epoch: 196, Loss: 0.0118\n","Epoch: 197, Loss: 0.0255\n","Epoch: 198, Loss: 0.0182\n","Epoch: 199, Loss: 0.0238\n","Epoch: 200, Loss: 0.0311\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"64C_wzE8L33c","executionInfo":{"status":"ok","timestamp":1646054578794,"user_tz":-60,"elapsed":257,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"dc7e672b-d209-4421-ae89-d34b0e2f2b99"},"source":["test_acc,attn_output_weights = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.6540\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dz64QTXV54Gs","executionInfo":{"status":"ok","timestamp":1646054581139,"user_tz":-60,"elapsed":196,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"c4ff6eda-7f37-4310-cd7b-ee9f792dea08"},"source":["data.edge_index[:,0:10]"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[   0,    1,    1,    1,    1,    1,    2,    3,    3,    4],\n","        [ 628,  158,  486, 1097, 2919, 2933, 3285, 1431, 3219,  467]],\n","       device='cuda:0')"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["print(attn_output_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M0IcENxH0UkX","executionInfo":{"status":"ok","timestamp":1646054583171,"user_tz":-60,"elapsed":217,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"bc547fcf-e399-40e0-f47a-38ca16af0050"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.5000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.1667, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.5000,  ..., 0.0000, 0.0000, 0.0000],\n","         ...,\n","         [0.0000, 0.0000, 0.0000,  ..., 0.2500, 0.0000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5000, 0.0000],\n","         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.5000]]],\n","       device='cuda:0', grad_fn=<DivBackward0>)\n"]}]},{"cell_type":"code","source":["def model_summary(model):\n","    \n","    model_params_list = list(model.named_parameters())\n","    print(\"----------------------------------------------------------------\")\n","    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer.Parameter\", \"Param Tensor Shape\", \"Param #\")\n","    print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    for elem in model_params_list:\n","        p_name = elem[0] \n","        p_shape = list(elem[1].size())\n","        p_count = torch.tensor(elem[1].size()).prod().item()\n","        line_new = \"{:>20}  {:>25} {:>15}\".format(p_name, str(p_shape), str(p_count))\n","        print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    total_params = sum([param.nelement() for param in model.parameters()])\n","    print(\"Total params:\", total_params)\n","    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"Trainable params:\", num_trainable_params)\n","    print(\"Non-trainable params:\", total_params - num_trainable_params)\n","\n","model_summary(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bFNai9u1e41","executionInfo":{"status":"ok","timestamp":1646054592623,"user_tz":-60,"elapsed":206,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"fd66c7d6-77e5-4861-b06c-5013bbc44c9b"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","     Layer.Parameter         Param Tensor Shape         Param #\n","----------------------------------------------------------------\n","         lin1.weight                [120, 3703]          444360\n","           lin1.bias                      [120]             120\n","attention1.in_proj_weight                 [360, 120]           43200\n","attention1.in_proj_bias                      [360]             360\n","attention1.out_proj.weight                 [120, 120]           14400\n","attention1.out_proj.bias                      [120]             120\n","         lin2.weight                   [6, 120]             720\n","           lin2.bias                        [6]               6\n","----------------------------------------------------------------\n","Total params: 503286\n","Trainable params: 503286\n","Non-trainable params: 0\n"]}]},{"cell_type":"markdown","source":["Self-attention to the neighboring nodes, accuracy 65.40% (training loss 0.01/0.02/0.03).\n","Comparing the model with the GCN (see below), we obtain less accuracy and less loss (0.02 aprox vs 0.4) with far more parameters (503286 vs 59366), probably because of overfitting. Using a dropout probability on attn_output_weights of 0.8 we get 63.1 % and 0.1/0.2 of training loss."],"metadata":{"id":"W0Hn5fJsrI4A"}},{"cell_type":"markdown","metadata":{"id":"XWSVkTn_FfGF"},"source":["**The basic model but in Cuda for comparisions**"]},{"cell_type":"code","metadata":{"id":"VCYTZ1eqEbhO"},"source":["import torch\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","\n","\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self, hidden_channels):\n","        super(GCN, self).__init__()\n","        torch.manual_seed(1234567)\n","        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","        x = self.conv1(x, edge_index)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return x\n","\n","model = GCN(hidden_channels=16)\n","\n","model.cuda()\n","data=data.cuda()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"lR1KVfvVEiZq","executionInfo":{"status":"ok","timestamp":1639047405255,"user_tz":-60,"elapsed":1413,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"f5df2912-8b9e-4646-879c-ba723f195fde"},"source":["from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out = model(data.x,data.edge_index)  # Perform a single forward pass.\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss\n","\n","def test():\n","      model.eval()\n","      out = model(data.x,data.edge_index)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc\n","\n","\n","for epoch in range(1, 201):\n","    loss = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 1.7915\n","Epoch: 002, Loss: 1.7875\n","Epoch: 003, Loss: 1.7823\n","Epoch: 004, Loss: 1.7752\n","Epoch: 005, Loss: 1.7683\n","Epoch: 006, Loss: 1.7605\n","Epoch: 007, Loss: 1.7525\n","Epoch: 008, Loss: 1.7423\n","Epoch: 009, Loss: 1.7387\n","Epoch: 010, Loss: 1.7282\n","Epoch: 011, Loss: 1.7168\n","Epoch: 012, Loss: 1.7133\n","Epoch: 013, Loss: 1.6932\n","Epoch: 014, Loss: 1.6868\n","Epoch: 015, Loss: 1.6759\n","Epoch: 016, Loss: 1.6629\n","Epoch: 017, Loss: 1.6535\n","Epoch: 018, Loss: 1.6446\n","Epoch: 019, Loss: 1.6305\n","Epoch: 020, Loss: 1.6317\n","Epoch: 021, Loss: 1.6121\n","Epoch: 022, Loss: 1.6036\n","Epoch: 023, Loss: 1.5931\n","Epoch: 024, Loss: 1.5798\n","Epoch: 025, Loss: 1.5575\n","Epoch: 026, Loss: 1.5395\n","Epoch: 027, Loss: 1.5365\n","Epoch: 028, Loss: 1.5098\n","Epoch: 029, Loss: 1.4833\n","Epoch: 030, Loss: 1.4816\n","Epoch: 031, Loss: 1.4661\n","Epoch: 032, Loss: 1.4568\n","Epoch: 033, Loss: 1.4259\n","Epoch: 034, Loss: 1.4174\n","Epoch: 035, Loss: 1.4302\n","Epoch: 036, Loss: 1.4050\n","Epoch: 037, Loss: 1.3624\n","Epoch: 038, Loss: 1.3600\n","Epoch: 039, Loss: 1.3464\n","Epoch: 040, Loss: 1.2972\n","Epoch: 041, Loss: 1.2932\n","Epoch: 042, Loss: 1.2640\n","Epoch: 043, Loss: 1.3010\n","Epoch: 044, Loss: 1.2461\n","Epoch: 045, Loss: 1.2579\n","Epoch: 046, Loss: 1.2150\n","Epoch: 047, Loss: 1.1948\n","Epoch: 048, Loss: 1.1877\n","Epoch: 049, Loss: 1.1802\n","Epoch: 050, Loss: 1.1640\n","Epoch: 051, Loss: 1.1240\n","Epoch: 052, Loss: 1.1319\n","Epoch: 053, Loss: 1.1034\n","Epoch: 054, Loss: 1.0958\n","Epoch: 055, Loss: 1.0653\n","Epoch: 056, Loss: 1.0779\n","Epoch: 057, Loss: 1.1009\n","Epoch: 058, Loss: 1.0206\n","Epoch: 059, Loss: 1.0315\n","Epoch: 060, Loss: 1.0150\n","Epoch: 061, Loss: 0.9945\n","Epoch: 062, Loss: 1.0052\n","Epoch: 063, Loss: 0.9563\n","Epoch: 064, Loss: 0.9470\n","Epoch: 065, Loss: 0.9453\n","Epoch: 066, Loss: 0.9658\n","Epoch: 067, Loss: 0.8888\n","Epoch: 068, Loss: 0.9505\n","Epoch: 069, Loss: 0.8900\n","Epoch: 070, Loss: 0.8760\n","Epoch: 071, Loss: 0.8373\n","Epoch: 072, Loss: 0.8740\n","Epoch: 073, Loss: 0.8816\n","Epoch: 074, Loss: 0.8558\n","Epoch: 075, Loss: 0.8019\n","Epoch: 076, Loss: 0.8260\n","Epoch: 077, Loss: 0.8505\n","Epoch: 078, Loss: 0.8272\n","Epoch: 079, Loss: 0.8141\n","Epoch: 080, Loss: 0.7889\n","Epoch: 081, Loss: 0.7788\n","Epoch: 082, Loss: 0.7280\n","Epoch: 083, Loss: 0.7378\n","Epoch: 084, Loss: 0.7622\n","Epoch: 085, Loss: 0.7702\n","Epoch: 086, Loss: 0.8006\n","Epoch: 087, Loss: 0.7354\n","Epoch: 088, Loss: 0.7113\n","Epoch: 089, Loss: 0.7172\n","Epoch: 090, Loss: 0.6920\n","Epoch: 091, Loss: 0.7217\n","Epoch: 092, Loss: 0.6746\n","Epoch: 093, Loss: 0.7152\n","Epoch: 094, Loss: 0.7302\n","Epoch: 095, Loss: 0.6796\n","Epoch: 096, Loss: 0.6606\n","Epoch: 097, Loss: 0.6642\n","Epoch: 098, Loss: 0.6642\n","Epoch: 099, Loss: 0.6851\n","Epoch: 100, Loss: 0.6636\n","Epoch: 101, Loss: 0.6177\n","Epoch: 102, Loss: 0.6245\n","Epoch: 103, Loss: 0.6095\n","Epoch: 104, Loss: 0.6181\n","Epoch: 105, Loss: 0.6079\n","Epoch: 106, Loss: 0.6221\n","Epoch: 107, Loss: 0.6271\n","Epoch: 108, Loss: 0.6452\n","Epoch: 109, Loss: 0.6114\n","Epoch: 110, Loss: 0.6071\n","Epoch: 111, Loss: 0.5959\n","Epoch: 112, Loss: 0.5926\n","Epoch: 113, Loss: 0.6016\n","Epoch: 114, Loss: 0.5942\n","Epoch: 115, Loss: 0.5663\n","Epoch: 116, Loss: 0.5433\n","Epoch: 117, Loss: 0.5747\n","Epoch: 118, Loss: 0.6145\n","Epoch: 119, Loss: 0.5651\n","Epoch: 120, Loss: 0.6027\n","Epoch: 121, Loss: 0.5679\n","Epoch: 122, Loss: 0.5412\n","Epoch: 123, Loss: 0.5613\n","Epoch: 124, Loss: 0.5629\n","Epoch: 125, Loss: 0.5262\n","Epoch: 126, Loss: 0.5450\n","Epoch: 127, Loss: 0.5472\n","Epoch: 128, Loss: 0.5443\n","Epoch: 129, Loss: 0.5515\n","Epoch: 130, Loss: 0.5662\n","Epoch: 131, Loss: 0.5156\n","Epoch: 132, Loss: 0.5297\n","Epoch: 133, Loss: 0.5539\n","Epoch: 134, Loss: 0.5504\n","Epoch: 135, Loss: 0.5485\n","Epoch: 136, Loss: 0.5345\n","Epoch: 137, Loss: 0.4880\n","Epoch: 138, Loss: 0.4787\n","Epoch: 139, Loss: 0.5319\n","Epoch: 140, Loss: 0.5130\n","Epoch: 141, Loss: 0.4945\n","Epoch: 142, Loss: 0.4686\n","Epoch: 143, Loss: 0.5053\n","Epoch: 144, Loss: 0.4832\n","Epoch: 145, Loss: 0.4842\n","Epoch: 146, Loss: 0.5055\n","Epoch: 147, Loss: 0.4849\n","Epoch: 148, Loss: 0.4947\n","Epoch: 149, Loss: 0.5094\n","Epoch: 150, Loss: 0.4725\n","Epoch: 151, Loss: 0.4761\n","Epoch: 152, Loss: 0.4698\n","Epoch: 153, Loss: 0.4658\n","Epoch: 154, Loss: 0.4809\n","Epoch: 155, Loss: 0.4642\n","Epoch: 156, Loss: 0.4690\n","Epoch: 157, Loss: 0.4769\n","Epoch: 158, Loss: 0.4554\n","Epoch: 159, Loss: 0.4907\n","Epoch: 160, Loss: 0.5107\n","Epoch: 161, Loss: 0.4553\n","Epoch: 162, Loss: 0.4410\n","Epoch: 163, Loss: 0.4547\n","Epoch: 164, Loss: 0.4309\n","Epoch: 165, Loss: 0.4221\n","Epoch: 166, Loss: 0.4606\n","Epoch: 167, Loss: 0.4778\n","Epoch: 168, Loss: 0.4821\n","Epoch: 169, Loss: 0.4155\n","Epoch: 170, Loss: 0.4278\n","Epoch: 171, Loss: 0.4190\n","Epoch: 172, Loss: 0.4528\n","Epoch: 173, Loss: 0.4219\n","Epoch: 174, Loss: 0.4290\n","Epoch: 175, Loss: 0.4412\n","Epoch: 176, Loss: 0.4818\n","Epoch: 177, Loss: 0.4446\n","Epoch: 178, Loss: 0.4397\n","Epoch: 179, Loss: 0.4413\n","Epoch: 180, Loss: 0.4390\n","Epoch: 181, Loss: 0.4516\n","Epoch: 182, Loss: 0.4324\n","Epoch: 183, Loss: 0.4009\n","Epoch: 184, Loss: 0.4110\n","Epoch: 185, Loss: 0.4230\n","Epoch: 186, Loss: 0.4189\n","Epoch: 187, Loss: 0.3886\n","Epoch: 188, Loss: 0.4305\n","Epoch: 189, Loss: 0.4454\n","Epoch: 190, Loss: 0.4351\n","Epoch: 191, Loss: 0.4292\n","Epoch: 192, Loss: 0.4393\n","Epoch: 193, Loss: 0.3926\n","Epoch: 194, Loss: 0.4831\n","Epoch: 195, Loss: 0.4343\n","Epoch: 196, Loss: 0.3775\n","Epoch: 197, Loss: 0.4321\n","Epoch: 198, Loss: 0.3740\n","Epoch: 199, Loss: 0.4082\n","Epoch: 200, Loss: 0.4241\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iwKHMjNYFJHH","executionInfo":{"status":"ok","timestamp":1639047409093,"user_tz":-60,"elapsed":352,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"97b2db74-ea5f-492a-a3ba-c9243d32d2a7"},"source":["test_acc = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7090\n"]}]},{"cell_type":"code","source":["def model_summary(model):\n","    \n","    model_params_list = list(model.named_parameters())\n","    print(\"----------------------------------------------------------------\")\n","    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer.Parameter\", \"Param Tensor Shape\", \"Param #\")\n","    print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    for elem in model_params_list:\n","        p_name = elem[0] \n","        p_shape = list(elem[1].size())\n","        p_count = torch.tensor(elem[1].size()).prod().item()\n","        line_new = \"{:>20}  {:>25} {:>15}\".format(p_name, str(p_shape), str(p_count))\n","        print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    total_params = sum([param.nelement() for param in model.parameters()])\n","    print(\"Total params:\", total_params)\n","    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"Trainable params:\", num_trainable_params)\n","    print(\"Non-trainable params:\", total_params - num_trainable_params)\n","\n","model_summary(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xF0HfNmp1mY3","executionInfo":{"status":"ok","timestamp":1639047412355,"user_tz":-60,"elapsed":10,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"8831cfcd-d6b4-4cd3-9778-c5defa48673c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","     Layer.Parameter         Param Tensor Shape         Param #\n","----------------------------------------------------------------\n","          conv1.bias                       [16]              16\n","    conv1.lin.weight                 [16, 3703]           59248\n","          conv2.bias                        [6]               6\n","    conv2.lin.weight                    [6, 16]              96\n","----------------------------------------------------------------\n","Total params: 59366\n","Trainable params: 59366\n","Non-trainable params: 0\n"]}]}]}