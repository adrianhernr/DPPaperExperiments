{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exp1CiteSeerGeneral.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN730Rpt8uoTzP3+OZnmosc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fYvxMt6P-g8e","executionInfo":{"status":"ok","timestamp":1644082366910,"user_tz":-60,"elapsed":8802,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"f2e67756-ffed-4a1e-fe6b-24f23fffa99d"},"source":["#Check the PyTorch and Cuda version\n","!python -c \"import torch; print(torch.__version__)\"\n","!python -c \"import torch; print(torch.version.cuda)\""],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n","11.1\n"]}]},{"cell_type":"code","metadata":{"id":"ICFCAiNp-qHq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644082396116,"user_tz":-60,"elapsed":29237,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"9e2dc7b4-30b8-4233-c2c4-4831e2fbfd5f"},"source":["!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install torch-cluster -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.5.9)\n","Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n","Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.7/dist-packages (1.2.1)\n"]}]},{"cell_type":"code","metadata":{"id":"JA1798K0-2Tt"},"source":["# Helper function for visualization.\n","%matplotlib inline\n","import torch\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","\n","\n","def visualize(h, color, epoch=None, loss=None):\n","    plt.figure(figsize=(7,7))\n","    plt.xticks([])\n","    plt.yticks([])\n","\n","    if torch.is_tensor(h):\n","        h = h.detach().cpu().numpy()\n","        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n","        if epoch is not None and loss is not None:\n","            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n","    else:\n","        nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n","                         node_color=color, cmap=\"Set2\")\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LgzQTkyL-4GJ","executionInfo":{"status":"ok","timestamp":1644082407841,"user_tz":-60,"elapsed":589,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"5bccfab8-9324-463c-8793-7537b87d6818"},"source":["#First data characteristics\n","from torch_geometric.datasets import Planetoid\n","from torch_geometric.transforms import NormalizeFeatures\n","\n","dataset = Planetoid(root='/tmp/CiteSeer', name='CiteSeer',transform=NormalizeFeatures())\n","\n","data = dataset[0]\n","print(data)\n","\n","print(f'Number of nodes: {data.num_nodes}')\n","print(f'Nodes features: {data.num_node_features}')\n","print(f'Number of classes: {dataset.num_classes}')\n","print(f'Number of edges: {data.num_edges}')\n","print(f'Avarage degree: {data.num_edges / data.num_nodes:.2f}')\n","print(f'Training nodes: {data.train_mask.sum()}')\n","print(f'Validation nodes: {data.val_mask.sum()}')\n","print(f'Test nodes: {data.test_mask.sum()}')\n","print(f'Isolated nodes: {data.has_isolated_nodes()}')\n","print(f'Loops: {data.has_self_loops()}')\n","print(f'Is undirected: {data.is_undirected()}')"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])\n","Number of nodes: 3327\n","Nodes features: 3703\n","Number of classes: 6\n","Number of edges: 9104\n","Avarage degree: 2.74\n","Training nodes: 120\n","Validation nodes: 500\n","Test nodes: 1000\n","Isolated nodes: True\n","Loops: False\n","Is undirected: True\n"]}]},{"cell_type":"code","metadata":{"id":"b1nmxakV_WHH"},"source":["#Converts a torch_geometric.data.Data instance to a networkx.Graph\n","from torch_geometric.utils import to_networkx\n","\n","G = to_networkx(data, to_undirected=True)\n","h=data.x, data.edge_index\n","#visualize(h, color=data.y)\n","\n","#Function to calculate the average degree. Change the number of nodes\n","def average_degree(G):\n","  degree=nx.degree(G)\n","  sum=0\n","  for i in range(3327):\n","    sum=sum+degree[i]\n","\n","  ad=sum/nx.number_of_nodes(G)\n","  return ad\n","\n","#Function to calculate the shortest path. Change the number of nodes. \n","\n","def average_path(G):\n","\n","  count=torch.empty(3328)\n","\n","  for j in range(2):\n","    sh=nx.shortest_path_length(G,source=j)\n","    count0=0\n","    contador=0\n","    for i in sh:\n","      count0=count0+sh[i]\n","      contador=contador+1\n","\n","    count[j]=count0/contador\n","    \n","\n","  shortestpath=torch.mean(count)\n","  return shortestpath\n","\n","shpath=average_path(G)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zy7XkmH2_a1E"},"source":["Dataset description: Citation network extracted from the CiteSeer digital library. Nodes are publications and the edges denote citations. https://networkrepository.com/citeseer.php"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FVSd1Kpf_fD6","executionInfo":{"status":"ok","timestamp":1644079228585,"user_tz":-60,"elapsed":226,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"4e54219e-d7cc-413a-856b-a22981fc1684"},"source":["#Data characteristics\n","#networkx functions and algorithms: degree, shortest paths, clustering, centrality, ...\n","print(f'Is weighted: {nx.is_weighted(G)}')\n","print(f'Is directed: {nx.is_directed(G)}')\n","print(f'Number of nodes: {nx.number_of_nodes(G)}')\n","print(f'Nodes features: {data.num_node_features}')\n","print(f'Number of classes: {dataset.num_classes}')\n","print(f'Number of edges: {nx.number_of_edges(G)}')\n","print(f'Density(num of edges vs maximal num of edges): {nx.density(G)}')\n","print(f'Average degree: {average_degree(G)}')\n","print(f'Is connected(every pair of nodes is connected): {nx.is_connected(G)}')\n","print(f'Average clustering coefficient: {nx.average_clustering(G)}')\n","print(f'Average shortest path: {shpath}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Is weighted: False\n","Is directed: False\n","Number of nodes: 3327\n","Nodes features: 3703\n","Number of classes: 6\n","Number of edges: 4552\n","Density(num of edges vs maximal num of edges): 0.0008227297529768376\n","Average degree: 2.7363991584009617\n","Is connected(every pair of nodes is connected): False\n","Average clustering coefficient: 0.14147102442629086\n","Average shortest path: 31.58768653869629\n"]}]},{"cell_type":"markdown","metadata":{"id":"wO0E6pBJAaUp"},"source":["**Model with only neural networks**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ps6nL8GAcof","executionInfo":{"status":"ok","timestamp":1644082448867,"user_tz":-60,"elapsed":245,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"992c32db-9a5d-496f-b3b7-1ea6e85c7bf9"},"source":["import torch\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","\n","\n","class MLP(torch.nn.Module):\n","    def __init__(self, hidden_channels):\n","        super(MLP, self).__init__()\n","        torch.manual_seed(12345)\n","        self.lin1 = Linear(dataset.num_features, hidden_channels)\n","        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n","\n","    def forward(self, x):\n","        x = self.lin1(x)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.lin2(x)\n","        return x\n","\n","model = MLP(hidden_channels=16)\n","def model_summary(model):\n","    \n","    model_params_list = list(model.named_parameters())\n","    print(\"----------------------------------------------------------------\")\n","    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer.Parameter\", \"Param Tensor Shape\", \"Param #\")\n","    print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    for elem in model_params_list:\n","        p_name = elem[0] \n","        p_shape = list(elem[1].size())\n","        p_count = torch.tensor(elem[1].size()).prod().item()\n","        line_new = \"{:>20}  {:>25} {:>15}\".format(p_name, str(p_shape), str(p_count))\n","        print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    total_params = sum([param.nelement() for param in model.parameters()])\n","    print(\"Total params:\", total_params)\n","    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"Trainable params:\", num_trainable_params)\n","    print(\"Non-trainable params:\", total_params - num_trainable_params)\n","\n","model_summary(model)"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","     Layer.Parameter         Param Tensor Shape         Param #\n","----------------------------------------------------------------\n","         lin1.weight                 [16, 3703]           59248\n","           lin1.bias                       [16]              16\n","         lin2.weight                    [6, 16]              96\n","           lin2.bias                        [6]               6\n","----------------------------------------------------------------\n","Total params: 59366\n","Trainable params: 59366\n","Non-trainable params: 0\n"]}]},{"cell_type":"code","metadata":{"id":"wHKs8inbA9yQ","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1644080324254,"user_tz":-60,"elapsed":5631,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"106fbe8a-2289-44ee-d075-1ed99bc480fc"},"source":["#For training, the cross entropy loss combines LogSoftmax and NLLLoss in one single class\n","#input is expected to contain raw, unnormalized scores for each class\n","#target a class index in the range for each value of a 1D tensor of size minibatch\n","#For testing, compares the class with highest probability with the labels and counts the correct predictions\n","from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","model = MLP(hidden_channels=16)\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out = model(data.x)  # Perform a single forward pass.\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss\n","\n","def test():\n","      model.eval()\n","      out = model(data.x)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc\n","\n","for epoch in range(1, 201):\n","    loss = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 1.8032\n","Epoch: 002, Loss: 1.7984\n","Epoch: 003, Loss: 1.7924\n","Epoch: 004, Loss: 1.7849\n","Epoch: 005, Loss: 1.7753\n","Epoch: 006, Loss: 1.7695\n","Epoch: 007, Loss: 1.7571\n","Epoch: 008, Loss: 1.7482\n","Epoch: 009, Loss: 1.7426\n","Epoch: 010, Loss: 1.7279\n","Epoch: 011, Loss: 1.7184\n","Epoch: 012, Loss: 1.6971\n","Epoch: 013, Loss: 1.6868\n","Epoch: 014, Loss: 1.6798\n","Epoch: 015, Loss: 1.6684\n","Epoch: 016, Loss: 1.6549\n","Epoch: 017, Loss: 1.6277\n","Epoch: 018, Loss: 1.6293\n","Epoch: 019, Loss: 1.6114\n","Epoch: 020, Loss: 1.5793\n","Epoch: 021, Loss: 1.5792\n","Epoch: 022, Loss: 1.5556\n","Epoch: 023, Loss: 1.5405\n","Epoch: 024, Loss: 1.5248\n","Epoch: 025, Loss: 1.4971\n","Epoch: 026, Loss: 1.4818\n","Epoch: 027, Loss: 1.4754\n","Epoch: 028, Loss: 1.4292\n","Epoch: 029, Loss: 1.4271\n","Epoch: 030, Loss: 1.4126\n","Epoch: 031, Loss: 1.3647\n","Epoch: 032, Loss: 1.3686\n","Epoch: 033, Loss: 1.3339\n","Epoch: 034, Loss: 1.3454\n","Epoch: 035, Loss: 1.2883\n","Epoch: 036, Loss: 1.3052\n","Epoch: 037, Loss: 1.2462\n","Epoch: 038, Loss: 1.2758\n","Epoch: 039, Loss: 1.1842\n","Epoch: 040, Loss: 1.1490\n","Epoch: 041, Loss: 1.1531\n","Epoch: 042, Loss: 1.1188\n","Epoch: 043, Loss: 1.1493\n","Epoch: 044, Loss: 1.0934\n","Epoch: 045, Loss: 1.1225\n","Epoch: 046, Loss: 1.0686\n","Epoch: 047, Loss: 1.0367\n","Epoch: 048, Loss: 1.0134\n","Epoch: 049, Loss: 0.9911\n","Epoch: 050, Loss: 1.0015\n","Epoch: 051, Loss: 0.9624\n","Epoch: 052, Loss: 0.9004\n","Epoch: 053, Loss: 0.9312\n","Epoch: 054, Loss: 0.8758\n","Epoch: 055, Loss: 0.8437\n","Epoch: 056, Loss: 0.8680\n","Epoch: 057, Loss: 0.8416\n","Epoch: 058, Loss: 0.8815\n","Epoch: 059, Loss: 0.8344\n","Epoch: 060, Loss: 0.8061\n","Epoch: 061, Loss: 0.8776\n","Epoch: 062, Loss: 0.7652\n","Epoch: 063, Loss: 0.7498\n","Epoch: 064, Loss: 0.7176\n","Epoch: 065, Loss: 0.7379\n","Epoch: 066, Loss: 0.7679\n","Epoch: 067, Loss: 0.6612\n","Epoch: 068, Loss: 0.6984\n","Epoch: 069, Loss: 0.6823\n","Epoch: 070, Loss: 0.7439\n","Epoch: 071, Loss: 0.6237\n","Epoch: 072, Loss: 0.6134\n","Epoch: 073, Loss: 0.6626\n","Epoch: 074, Loss: 0.6390\n","Epoch: 075, Loss: 0.6634\n","Epoch: 076, Loss: 0.6302\n","Epoch: 077, Loss: 0.6140\n","Epoch: 078, Loss: 0.6764\n","Epoch: 079, Loss: 0.6079\n","Epoch: 080, Loss: 0.5477\n","Epoch: 081, Loss: 0.5144\n","Epoch: 082, Loss: 0.5488\n","Epoch: 083, Loss: 0.5917\n","Epoch: 084, Loss: 0.6278\n","Epoch: 085, Loss: 0.6057\n","Epoch: 086, Loss: 0.6202\n","Epoch: 087, Loss: 0.4804\n","Epoch: 088, Loss: 0.5328\n","Epoch: 089, Loss: 0.5489\n","Epoch: 090, Loss: 0.5127\n","Epoch: 091, Loss: 0.5444\n","Epoch: 092, Loss: 0.5818\n","Epoch: 093, Loss: 0.4831\n","Epoch: 094, Loss: 0.5579\n","Epoch: 095, Loss: 0.5032\n","Epoch: 096, Loss: 0.5278\n","Epoch: 097, Loss: 0.4652\n","Epoch: 098, Loss: 0.5292\n","Epoch: 099, Loss: 0.5748\n","Epoch: 100, Loss: 0.5078\n","Epoch: 101, Loss: 0.4361\n","Epoch: 102, Loss: 0.5125\n","Epoch: 103, Loss: 0.4820\n","Epoch: 104, Loss: 0.4802\n","Epoch: 105, Loss: 0.4731\n","Epoch: 106, Loss: 0.5326\n","Epoch: 107, Loss: 0.4918\n","Epoch: 108, Loss: 0.4732\n","Epoch: 109, Loss: 0.5069\n","Epoch: 110, Loss: 0.4658\n","Epoch: 111, Loss: 0.4502\n","Epoch: 112, Loss: 0.5123\n","Epoch: 113, Loss: 0.4542\n","Epoch: 114, Loss: 0.4683\n","Epoch: 115, Loss: 0.4664\n","Epoch: 116, Loss: 0.4151\n","Epoch: 117, Loss: 0.4517\n","Epoch: 118, Loss: 0.4175\n","Epoch: 119, Loss: 0.4873\n","Epoch: 120, Loss: 0.4191\n","Epoch: 121, Loss: 0.4398\n","Epoch: 122, Loss: 0.4270\n","Epoch: 123, Loss: 0.5053\n","Epoch: 124, Loss: 0.4281\n","Epoch: 125, Loss: 0.5353\n","Epoch: 126, Loss: 0.4332\n","Epoch: 127, Loss: 0.4396\n","Epoch: 128, Loss: 0.4369\n","Epoch: 129, Loss: 0.4962\n","Epoch: 130, Loss: 0.4491\n","Epoch: 131, Loss: 0.4345\n","Epoch: 132, Loss: 0.4535\n","Epoch: 133, Loss: 0.4801\n","Epoch: 134, Loss: 0.4789\n","Epoch: 135, Loss: 0.3900\n","Epoch: 136, Loss: 0.4103\n","Epoch: 137, Loss: 0.4875\n","Epoch: 138, Loss: 0.4212\n","Epoch: 139, Loss: 0.4655\n","Epoch: 140, Loss: 0.4426\n","Epoch: 141, Loss: 0.4022\n","Epoch: 142, Loss: 0.4243\n","Epoch: 143, Loss: 0.4288\n","Epoch: 144, Loss: 0.4227\n","Epoch: 145, Loss: 0.4991\n","Epoch: 146, Loss: 0.3758\n","Epoch: 147, Loss: 0.4061\n","Epoch: 148, Loss: 0.3529\n","Epoch: 149, Loss: 0.3686\n","Epoch: 150, Loss: 0.4146\n","Epoch: 151, Loss: 0.3836\n","Epoch: 152, Loss: 0.4203\n","Epoch: 153, Loss: 0.4591\n","Epoch: 154, Loss: 0.4231\n","Epoch: 155, Loss: 0.3670\n","Epoch: 156, Loss: 0.4860\n","Epoch: 157, Loss: 0.4142\n","Epoch: 158, Loss: 0.4011\n","Epoch: 159, Loss: 0.4194\n","Epoch: 160, Loss: 0.4921\n","Epoch: 161, Loss: 0.4020\n","Epoch: 162, Loss: 0.3797\n","Epoch: 163, Loss: 0.3409\n","Epoch: 164, Loss: 0.4180\n","Epoch: 165, Loss: 0.4364\n","Epoch: 166, Loss: 0.3700\n","Epoch: 167, Loss: 0.4699\n","Epoch: 168, Loss: 0.3767\n","Epoch: 169, Loss: 0.3874\n","Epoch: 170, Loss: 0.3707\n","Epoch: 171, Loss: 0.4087\n","Epoch: 172, Loss: 0.4178\n","Epoch: 173, Loss: 0.3996\n","Epoch: 174, Loss: 0.3929\n","Epoch: 175, Loss: 0.4146\n","Epoch: 176, Loss: 0.4156\n","Epoch: 177, Loss: 0.3877\n","Epoch: 178, Loss: 0.3807\n","Epoch: 179, Loss: 0.4097\n","Epoch: 180, Loss: 0.3892\n","Epoch: 181, Loss: 0.4143\n","Epoch: 182, Loss: 0.3476\n","Epoch: 183, Loss: 0.3734\n","Epoch: 184, Loss: 0.4284\n","Epoch: 185, Loss: 0.4550\n","Epoch: 186, Loss: 0.3613\n","Epoch: 187, Loss: 0.3996\n","Epoch: 188, Loss: 0.3849\n","Epoch: 189, Loss: 0.4075\n","Epoch: 190, Loss: 0.4187\n","Epoch: 191, Loss: 0.4234\n","Epoch: 192, Loss: 0.4621\n","Epoch: 193, Loss: 0.3743\n","Epoch: 194, Loss: 0.3258\n","Epoch: 195, Loss: 0.3411\n","Epoch: 196, Loss: 0.3923\n","Epoch: 197, Loss: 0.3520\n","Epoch: 198, Loss: 0.3917\n","Epoch: 199, Loss: 0.4151\n","Epoch: 200, Loss: 0.3789\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nz9ucEfkBL7T","executionInfo":{"status":"ok","timestamp":1644080329177,"user_tz":-60,"elapsed":251,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"4bcb9218-1e3d-40f9-d1de-700afc32cf20"},"source":["test_acc = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.5820\n"]}]},{"cell_type":"markdown","metadata":{"id":"TVkFa5bZBQfb"},"source":["MLP performs with only about 58% test accuracy. This model suffers from heavy overfitting due to only a small amount of training nodes, and therefore generalizes poorly to unseen node representations.\n","\n","It also fails to incorporate an important bias into the model: Cited papers are very likely related to the category of a document. \n"]},{"cell_type":"markdown","metadata":{"id":"BGedQLDOptR2"},"source":["**Model with neural networks, more layers and intermediate dimension**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUOaW1Zqpzqu","executionInfo":{"status":"ok","timestamp":1644080496533,"user_tz":-60,"elapsed":231,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"ddd14b63-a344-4d19-b1d2-18dcb6205184"},"source":["import torch\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","\n","\n","class MLP(torch.nn.Module):\n","    def __init__(self, hidden_channels1,hidden_channels2):\n","        super(MLP, self).__init__()\n","        torch.manual_seed(12345)\n","        self.lin1 = Linear(dataset.num_features, hidden_channels1)\n","        self.lin2 = Linear(hidden_channels1, hidden_channels2)\n","        self.lin3 = Linear(hidden_channels2, dataset.num_classes)\n","\n","    def forward(self, x):\n","        x = self.lin1(x)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.lin2(x)\n","        x = x.relu()\n","        x = self.lin3(x)\n","        return x\n","\n","model = MLP(hidden_channels1=512,hidden_channels2=256)\n","\n","def model_summary(model):\n","    \n","    model_params_list = list(model.named_parameters())\n","    print(\"----------------------------------------------------------------\")\n","    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer.Parameter\", \"Param Tensor Shape\", \"Param #\")\n","    print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    for elem in model_params_list:\n","        p_name = elem[0] \n","        p_shape = list(elem[1].size())\n","        p_count = torch.tensor(elem[1].size()).prod().item()\n","        line_new = \"{:>20}  {:>25} {:>15}\".format(p_name, str(p_shape), str(p_count))\n","        print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    total_params = sum([param.nelement() for param in model.parameters()])\n","    print(\"Total params:\", total_params)\n","    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"Trainable params:\", num_trainable_params)\n","    print(\"Non-trainable params:\", total_params - num_trainable_params)\n","\n","model_summary(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","     Layer.Parameter         Param Tensor Shape         Param #\n","----------------------------------------------------------------\n","         lin1.weight                [512, 3703]         1895936\n","           lin1.bias                      [512]             512\n","         lin2.weight                 [256, 512]          131072\n","           lin2.bias                      [256]             256\n","         lin3.weight                   [6, 256]            1536\n","           lin3.bias                        [6]               6\n","----------------------------------------------------------------\n","Total params: 2029318\n","Trainable params: 2029318\n","Non-trainable params: 0\n"]}]},{"cell_type":"code","metadata":{"id":"cqwVOw2Rsdep","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1644080587357,"user_tz":-60,"elapsed":86102,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"a65eac8a-0838-49c7-c36a-3a07d16f9c4c"},"source":["#For training, the cross entropy loss combines LogSoftmax and NLLLoss in one single class\n","#input is expected to contain raw, unnormalized scores for each class\n","#target a class index in the range for each value of a 1D tensor of size minibatch\n","#For testing, compares the class with highest probability with the labels and counts the correct predictions\n","from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","model = MLP(hidden_channels1=512,hidden_channels2=256)\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out = model(data.x)  # Perform a single forward pass.\n","      loss = criterion(out[data.val_mask], data.y[data.val_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss\n","\n","def test():\n","      model.eval()\n","      out = model(data.x)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc\n","\n","for epoch in range(1, 201):\n","    loss = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 1.7972\n","Epoch: 002, Loss: 1.7675\n","Epoch: 003, Loss: 1.7334\n","Epoch: 004, Loss: 1.7187\n","Epoch: 005, Loss: 1.6949\n","Epoch: 006, Loss: 1.6758\n","Epoch: 007, Loss: 1.6509\n","Epoch: 008, Loss: 1.6078\n","Epoch: 009, Loss: 1.5508\n","Epoch: 010, Loss: 1.4814\n","Epoch: 011, Loss: 1.3886\n","Epoch: 012, Loss: 1.2802\n","Epoch: 013, Loss: 1.1697\n","Epoch: 014, Loss: 1.0366\n","Epoch: 015, Loss: 0.8982\n","Epoch: 016, Loss: 0.7681\n","Epoch: 017, Loss: 0.6300\n","Epoch: 018, Loss: 0.5032\n","Epoch: 019, Loss: 0.4057\n","Epoch: 020, Loss: 0.3202\n","Epoch: 021, Loss: 0.2661\n","Epoch: 022, Loss: 0.1994\n","Epoch: 023, Loss: 0.1682\n","Epoch: 024, Loss: 0.1379\n","Epoch: 025, Loss: 0.1252\n","Epoch: 026, Loss: 0.0940\n","Epoch: 027, Loss: 0.0890\n","Epoch: 028, Loss: 0.0736\n","Epoch: 029, Loss: 0.0732\n","Epoch: 030, Loss: 0.0657\n","Epoch: 031, Loss: 0.0620\n","Epoch: 032, Loss: 0.0612\n","Epoch: 033, Loss: 0.0693\n","Epoch: 034, Loss: 0.0694\n","Epoch: 035, Loss: 0.0590\n","Epoch: 036, Loss: 0.0595\n","Epoch: 037, Loss: 0.0656\n","Epoch: 038, Loss: 0.0536\n","Epoch: 039, Loss: 0.0525\n","Epoch: 040, Loss: 0.0462\n","Epoch: 041, Loss: 0.0470\n","Epoch: 042, Loss: 0.0489\n","Epoch: 043, Loss: 0.0474\n","Epoch: 044, Loss: 0.0432\n","Epoch: 045, Loss: 0.0448\n","Epoch: 046, Loss: 0.0443\n","Epoch: 047, Loss: 0.0447\n","Epoch: 048, Loss: 0.0444\n","Epoch: 049, Loss: 0.0483\n","Epoch: 050, Loss: 0.0475\n","Epoch: 051, Loss: 0.0457\n","Epoch: 052, Loss: 0.0464\n","Epoch: 053, Loss: 0.0452\n","Epoch: 054, Loss: 0.0510\n","Epoch: 055, Loss: 0.0453\n","Epoch: 056, Loss: 0.0477\n","Epoch: 057, Loss: 0.0503\n","Epoch: 058, Loss: 0.0457\n","Epoch: 059, Loss: 0.0485\n","Epoch: 060, Loss: 0.0413\n","Epoch: 061, Loss: 0.0440\n","Epoch: 062, Loss: 0.0390\n","Epoch: 063, Loss: 0.0478\n","Epoch: 064, Loss: 0.0425\n","Epoch: 065, Loss: 0.0407\n","Epoch: 066, Loss: 0.0415\n","Epoch: 067, Loss: 0.0422\n","Epoch: 068, Loss: 0.0397\n","Epoch: 069, Loss: 0.0406\n","Epoch: 070, Loss: 0.0396\n","Epoch: 071, Loss: 0.0416\n","Epoch: 072, Loss: 0.0447\n","Epoch: 073, Loss: 0.0436\n","Epoch: 074, Loss: 0.0390\n","Epoch: 075, Loss: 0.0351\n","Epoch: 076, Loss: 0.0354\n","Epoch: 077, Loss: 0.0369\n","Epoch: 078, Loss: 0.0331\n","Epoch: 079, Loss: 0.0352\n","Epoch: 080, Loss: 0.0350\n","Epoch: 081, Loss: 0.0339\n","Epoch: 082, Loss: 0.0348\n","Epoch: 083, Loss: 0.0306\n","Epoch: 084, Loss: 0.0350\n","Epoch: 085, Loss: 0.0324\n","Epoch: 086, Loss: 0.0349\n","Epoch: 087, Loss: 0.0319\n","Epoch: 088, Loss: 0.0343\n","Epoch: 089, Loss: 0.0332\n","Epoch: 090, Loss: 0.0346\n","Epoch: 091, Loss: 0.0318\n","Epoch: 092, Loss: 0.0322\n","Epoch: 093, Loss: 0.0347\n","Epoch: 094, Loss: 0.0277\n","Epoch: 095, Loss: 0.0311\n","Epoch: 096, Loss: 0.0294\n","Epoch: 097, Loss: 0.0312\n","Epoch: 098, Loss: 0.0315\n","Epoch: 099, Loss: 0.0325\n","Epoch: 100, Loss: 0.0280\n","Epoch: 101, Loss: 0.0269\n","Epoch: 102, Loss: 0.0276\n","Epoch: 103, Loss: 0.0301\n","Epoch: 104, Loss: 0.0319\n","Epoch: 105, Loss: 0.0283\n","Epoch: 106, Loss: 0.0314\n","Epoch: 107, Loss: 0.0315\n","Epoch: 108, Loss: 0.0261\n","Epoch: 109, Loss: 0.0294\n","Epoch: 110, Loss: 0.0313\n","Epoch: 111, Loss: 0.0335\n","Epoch: 112, Loss: 0.0263\n","Epoch: 113, Loss: 0.0283\n","Epoch: 114, Loss: 0.0265\n","Epoch: 115, Loss: 0.0275\n","Epoch: 116, Loss: 0.0268\n","Epoch: 117, Loss: 0.0255\n","Epoch: 118, Loss: 0.0280\n","Epoch: 119, Loss: 0.0298\n","Epoch: 120, Loss: 0.0274\n","Epoch: 121, Loss: 0.0247\n","Epoch: 122, Loss: 0.0277\n","Epoch: 123, Loss: 0.0286\n","Epoch: 124, Loss: 0.0247\n","Epoch: 125, Loss: 0.0245\n","Epoch: 126, Loss: 0.0256\n","Epoch: 127, Loss: 0.0287\n","Epoch: 128, Loss: 0.0270\n","Epoch: 129, Loss: 0.0264\n","Epoch: 130, Loss: 0.0266\n","Epoch: 131, Loss: 0.0278\n","Epoch: 132, Loss: 0.0279\n","Epoch: 133, Loss: 0.0278\n","Epoch: 134, Loss: 0.0250\n","Epoch: 135, Loss: 0.0237\n","Epoch: 136, Loss: 0.0260\n","Epoch: 137, Loss: 0.0231\n","Epoch: 138, Loss: 0.0272\n","Epoch: 139, Loss: 0.0247\n","Epoch: 140, Loss: 0.0254\n","Epoch: 141, Loss: 0.0257\n","Epoch: 142, Loss: 0.0258\n","Epoch: 143, Loss: 0.0284\n","Epoch: 144, Loss: 0.0267\n","Epoch: 145, Loss: 0.0247\n","Epoch: 146, Loss: 0.0272\n","Epoch: 147, Loss: 0.0258\n","Epoch: 148, Loss: 0.0241\n","Epoch: 149, Loss: 0.0252\n","Epoch: 150, Loss: 0.0240\n","Epoch: 151, Loss: 0.0252\n","Epoch: 152, Loss: 0.0237\n","Epoch: 153, Loss: 0.0243\n","Epoch: 154, Loss: 0.0213\n","Epoch: 155, Loss: 0.0282\n","Epoch: 156, Loss: 0.0289\n","Epoch: 157, Loss: 0.0268\n","Epoch: 158, Loss: 0.0260\n","Epoch: 159, Loss: 0.0256\n","Epoch: 160, Loss: 0.0277\n","Epoch: 161, Loss: 0.0213\n","Epoch: 162, Loss: 0.0226\n","Epoch: 163, Loss: 0.0244\n","Epoch: 164, Loss: 0.0211\n","Epoch: 165, Loss: 0.0233\n","Epoch: 166, Loss: 0.0270\n","Epoch: 167, Loss: 0.0236\n","Epoch: 168, Loss: 0.0233\n","Epoch: 169, Loss: 0.0258\n","Epoch: 170, Loss: 0.0242\n","Epoch: 171, Loss: 0.0260\n","Epoch: 172, Loss: 0.0245\n","Epoch: 173, Loss: 0.0236\n","Epoch: 174, Loss: 0.0263\n","Epoch: 175, Loss: 0.0236\n","Epoch: 176, Loss: 0.0247\n","Epoch: 177, Loss: 0.0241\n","Epoch: 178, Loss: 0.0236\n","Epoch: 179, Loss: 0.0211\n","Epoch: 180, Loss: 0.0236\n","Epoch: 181, Loss: 0.0208\n","Epoch: 182, Loss: 0.0235\n","Epoch: 183, Loss: 0.0233\n","Epoch: 184, Loss: 0.0253\n","Epoch: 185, Loss: 0.0278\n","Epoch: 186, Loss: 0.0271\n","Epoch: 187, Loss: 0.0249\n","Epoch: 188, Loss: 0.0231\n","Epoch: 189, Loss: 0.0240\n","Epoch: 190, Loss: 0.0258\n","Epoch: 191, Loss: 0.0230\n","Epoch: 192, Loss: 0.0234\n","Epoch: 193, Loss: 0.0209\n","Epoch: 194, Loss: 0.0235\n","Epoch: 195, Loss: 0.0248\n","Epoch: 196, Loss: 0.0261\n","Epoch: 197, Loss: 0.0223\n","Epoch: 198, Loss: 0.0234\n","Epoch: 199, Loss: 0.0220\n","Epoch: 200, Loss: 0.0229\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVXwTBhmsp_4","executionInfo":{"status":"ok","timestamp":1644080589310,"user_tz":-60,"elapsed":226,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"72df42db-1720-4ad1-d45c-848d956bb430"},"source":["test_acc = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.6770\n"]}]},{"cell_type":"markdown","metadata":{"id":"K8pxOhk-xlu2"},"source":["With a MLP, with two intermediate layers, intermediate dimensions 512 and 256, 2,029,318 of parameters and training with the training mask, we get test accuracy of 59%, slightly above the 58% of the previous configuration. Probably, the model is overfitting the training data. Using the validation mask to train the model, the loss decrease more slowly and gets 68% of test accuracy."]},{"cell_type":"markdown","metadata":{"id":"aOesxumADWtu"},"source":["**Model with GCN**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Wt9BL5YBUlg","executionInfo":{"status":"ok","timestamp":1644081472080,"user_tz":-60,"elapsed":621,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"9aba3142-f67b-47c7-f67f-cd0f21cfe405"},"source":["from torch_geometric.nn import GCNConv\n","\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self, hidden_channels):\n","        super(GCN, self).__init__()\n","        torch.manual_seed(1234567)\n","        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","        x = self.conv1(x, edge_index)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return x\n","\n","model = GCN(hidden_channels=16)\n","\n","def model_summary(model):\n","    \n","    model_params_list = list(model.named_parameters())\n","    print(\"----------------------------------------------------------------\")\n","    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer.Parameter\", \"Param Tensor Shape\", \"Param #\")\n","    print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    for elem in model_params_list:\n","        p_name = elem[0] \n","        p_shape = list(elem[1].size())\n","        p_count = torch.tensor(elem[1].size()).prod().item()\n","        line_new = \"{:>20}  {:>25} {:>15}\".format(p_name, str(p_shape), str(p_count))\n","        print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    total_params = sum([param.nelement() for param in model.parameters()])\n","    print(\"Total params:\", total_params)\n","    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"Trainable params:\", num_trainable_params)\n","    print(\"Non-trainable params:\", total_params - num_trainable_params)\n","\n","model_summary(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","     Layer.Parameter         Param Tensor Shape         Param #\n","----------------------------------------------------------------\n","          conv1.bias                       [16]              16\n","    conv1.lin.weight                 [16, 3703]           59248\n","          conv2.bias                        [6]               6\n","    conv2.lin.weight                    [6, 16]              96\n","----------------------------------------------------------------\n","Total params: 59366\n","Trainable params: 59366\n","Non-trainable params: 0\n"]}]},{"cell_type":"code","metadata":{"id":"X1JRgje7BZNi","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1644081485669,"user_tz":-60,"elapsed":7291,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"bcc50633-16c3-4b45-9082-c5a1b2122692"},"source":["from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","model = GCN(hidden_channels=16)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n","      loss = criterion(out[data.val_mask], data.y[data.val_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss\n","\n","def test():\n","      model.eval()\n","      out = model(data.x, data.edge_index)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc\n","\n","\n","for epoch in range(1, 201):\n","    loss = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 1.7916\n","Epoch: 002, Loss: 1.7839\n","Epoch: 003, Loss: 1.7756\n","Epoch: 004, Loss: 1.7676\n","Epoch: 005, Loss: 1.7593\n","Epoch: 006, Loss: 1.7479\n","Epoch: 007, Loss: 1.7415\n","Epoch: 008, Loss: 1.7316\n","Epoch: 009, Loss: 1.7228\n","Epoch: 010, Loss: 1.7151\n","Epoch: 011, Loss: 1.7045\n","Epoch: 012, Loss: 1.6994\n","Epoch: 013, Loss: 1.6930\n","Epoch: 014, Loss: 1.6847\n","Epoch: 015, Loss: 1.6747\n","Epoch: 016, Loss: 1.6705\n","Epoch: 017, Loss: 1.6618\n","Epoch: 018, Loss: 1.6522\n","Epoch: 019, Loss: 1.6474\n","Epoch: 020, Loss: 1.6411\n","Epoch: 021, Loss: 1.6394\n","Epoch: 022, Loss: 1.6350\n","Epoch: 023, Loss: 1.6225\n","Epoch: 024, Loss: 1.6085\n","Epoch: 025, Loss: 1.6051\n","Epoch: 026, Loss: 1.6024\n","Epoch: 027, Loss: 1.5851\n","Epoch: 028, Loss: 1.5862\n","Epoch: 029, Loss: 1.5702\n","Epoch: 030, Loss: 1.5571\n","Epoch: 031, Loss: 1.5520\n","Epoch: 032, Loss: 1.5399\n","Epoch: 033, Loss: 1.5422\n","Epoch: 034, Loss: 1.5373\n","Epoch: 035, Loss: 1.5141\n","Epoch: 036, Loss: 1.5011\n","Epoch: 037, Loss: 1.4898\n","Epoch: 038, Loss: 1.4844\n","Epoch: 039, Loss: 1.4731\n","Epoch: 040, Loss: 1.4689\n","Epoch: 041, Loss: 1.4555\n","Epoch: 042, Loss: 1.4493\n","Epoch: 043, Loss: 1.4323\n","Epoch: 044, Loss: 1.4151\n","Epoch: 045, Loss: 1.4156\n","Epoch: 046, Loss: 1.3993\n","Epoch: 047, Loss: 1.3999\n","Epoch: 048, Loss: 1.3710\n","Epoch: 049, Loss: 1.3683\n","Epoch: 050, Loss: 1.3622\n","Epoch: 051, Loss: 1.3343\n","Epoch: 052, Loss: 1.3349\n","Epoch: 053, Loss: 1.3213\n","Epoch: 054, Loss: 1.3125\n","Epoch: 055, Loss: 1.3042\n","Epoch: 056, Loss: 1.2871\n","Epoch: 057, Loss: 1.2776\n","Epoch: 058, Loss: 1.2856\n","Epoch: 059, Loss: 1.2554\n","Epoch: 060, Loss: 1.2560\n","Epoch: 061, Loss: 1.2416\n","Epoch: 062, Loss: 1.2249\n","Epoch: 063, Loss: 1.2207\n","Epoch: 064, Loss: 1.2230\n","Epoch: 065, Loss: 1.1995\n","Epoch: 066, Loss: 1.1996\n","Epoch: 067, Loss: 1.1938\n","Epoch: 068, Loss: 1.1540\n","Epoch: 069, Loss: 1.1904\n","Epoch: 070, Loss: 1.1633\n","Epoch: 071, Loss: 1.1389\n","Epoch: 072, Loss: 1.1400\n","Epoch: 073, Loss: 1.1344\n","Epoch: 074, Loss: 1.1137\n","Epoch: 075, Loss: 1.1113\n","Epoch: 076, Loss: 1.1049\n","Epoch: 077, Loss: 1.0808\n","Epoch: 078, Loss: 1.0893\n","Epoch: 079, Loss: 1.0764\n","Epoch: 080, Loss: 1.0816\n","Epoch: 081, Loss: 1.0584\n","Epoch: 082, Loss: 1.0643\n","Epoch: 083, Loss: 1.0371\n","Epoch: 084, Loss: 1.0395\n","Epoch: 085, Loss: 1.0511\n","Epoch: 086, Loss: 1.0268\n","Epoch: 087, Loss: 1.0180\n","Epoch: 088, Loss: 1.0195\n","Epoch: 089, Loss: 1.0096\n","Epoch: 090, Loss: 1.0117\n","Epoch: 091, Loss: 0.9998\n","Epoch: 092, Loss: 0.9945\n","Epoch: 093, Loss: 0.9979\n","Epoch: 094, Loss: 0.9769\n","Epoch: 095, Loss: 0.9479\n","Epoch: 096, Loss: 0.9676\n","Epoch: 097, Loss: 0.9616\n","Epoch: 098, Loss: 0.9332\n","Epoch: 099, Loss: 0.9487\n","Epoch: 100, Loss: 0.9179\n","Epoch: 101, Loss: 0.9399\n","Epoch: 102, Loss: 0.9271\n","Epoch: 103, Loss: 0.9408\n","Epoch: 104, Loss: 0.9277\n","Epoch: 105, Loss: 0.9155\n","Epoch: 106, Loss: 0.9239\n","Epoch: 107, Loss: 0.8915\n","Epoch: 108, Loss: 0.9034\n","Epoch: 109, Loss: 0.8855\n","Epoch: 110, Loss: 0.8807\n","Epoch: 111, Loss: 0.8690\n","Epoch: 112, Loss: 0.8966\n","Epoch: 113, Loss: 0.8980\n","Epoch: 114, Loss: 0.8904\n","Epoch: 115, Loss: 0.8818\n","Epoch: 116, Loss: 0.8607\n","Epoch: 117, Loss: 0.8815\n","Epoch: 118, Loss: 0.8514\n","Epoch: 119, Loss: 0.8295\n","Epoch: 120, Loss: 0.8708\n","Epoch: 121, Loss: 0.8203\n","Epoch: 122, Loss: 0.8412\n","Epoch: 123, Loss: 0.8474\n","Epoch: 124, Loss: 0.8445\n","Epoch: 125, Loss: 0.8207\n","Epoch: 126, Loss: 0.8013\n","Epoch: 127, Loss: 0.8052\n","Epoch: 128, Loss: 0.8110\n","Epoch: 129, Loss: 0.8217\n","Epoch: 130, Loss: 0.8229\n","Epoch: 131, Loss: 0.7918\n","Epoch: 132, Loss: 0.7923\n","Epoch: 133, Loss: 0.8198\n","Epoch: 134, Loss: 0.7804\n","Epoch: 135, Loss: 0.7946\n","Epoch: 136, Loss: 0.7761\n","Epoch: 137, Loss: 0.7914\n","Epoch: 138, Loss: 0.8137\n","Epoch: 139, Loss: 0.7977\n","Epoch: 140, Loss: 0.7559\n","Epoch: 141, Loss: 0.7740\n","Epoch: 142, Loss: 0.7652\n","Epoch: 143, Loss: 0.7633\n","Epoch: 144, Loss: 0.7606\n","Epoch: 145, Loss: 0.7655\n","Epoch: 146, Loss: 0.7564\n","Epoch: 147, Loss: 0.7283\n","Epoch: 148, Loss: 0.7374\n","Epoch: 149, Loss: 0.7525\n","Epoch: 150, Loss: 0.7168\n","Epoch: 151, Loss: 0.7522\n","Epoch: 152, Loss: 0.7528\n","Epoch: 153, Loss: 0.7557\n","Epoch: 154, Loss: 0.7409\n","Epoch: 155, Loss: 0.7391\n","Epoch: 156, Loss: 0.7136\n","Epoch: 157, Loss: 0.7568\n","Epoch: 158, Loss: 0.7057\n","Epoch: 159, Loss: 0.7196\n","Epoch: 160, Loss: 0.7082\n","Epoch: 161, Loss: 0.6907\n","Epoch: 162, Loss: 0.7326\n","Epoch: 163, Loss: 0.7173\n","Epoch: 164, Loss: 0.7337\n","Epoch: 165, Loss: 0.7384\n","Epoch: 166, Loss: 0.7189\n","Epoch: 167, Loss: 0.6979\n","Epoch: 168, Loss: 0.6955\n","Epoch: 169, Loss: 0.7048\n","Epoch: 170, Loss: 0.6857\n","Epoch: 171, Loss: 0.6947\n","Epoch: 172, Loss: 0.6997\n","Epoch: 173, Loss: 0.6855\n","Epoch: 174, Loss: 0.6738\n","Epoch: 175, Loss: 0.6936\n","Epoch: 176, Loss: 0.6885\n","Epoch: 177, Loss: 0.6613\n","Epoch: 178, Loss: 0.7021\n","Epoch: 179, Loss: 0.6853\n","Epoch: 180, Loss: 0.6742\n","Epoch: 181, Loss: 0.6765\n","Epoch: 182, Loss: 0.6718\n","Epoch: 183, Loss: 0.6660\n","Epoch: 184, Loss: 0.6742\n","Epoch: 185, Loss: 0.6615\n","Epoch: 186, Loss: 0.6662\n","Epoch: 187, Loss: 0.6537\n","Epoch: 188, Loss: 0.6698\n","Epoch: 189, Loss: 0.6715\n","Epoch: 190, Loss: 0.6459\n","Epoch: 191, Loss: 0.6564\n","Epoch: 192, Loss: 0.6525\n","Epoch: 193, Loss: 0.6435\n","Epoch: 194, Loss: 0.6534\n","Epoch: 195, Loss: 0.6645\n","Epoch: 196, Loss: 0.6500\n","Epoch: 197, Loss: 0.6382\n","Epoch: 198, Loss: 0.6574\n","Epoch: 199, Loss: 0.6284\n","Epoch: 200, Loss: 0.6279\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0tNtjYH3Coi6","executionInfo":{"status":"ok","timestamp":1644081490214,"user_tz":-60,"elapsed":246,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"2143825a-4d7a-48f3-ba2b-e4ad466fcc48"},"source":["test_acc = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7620\n"]}]},{"cell_type":"markdown","metadata":{"id":"7mTfEBLzzxlj"},"source":["Using a GCN with a GCN layer and 59,366 parameters, we get a test accuracy of 71.4% (the training loss is 0.44). If we use the validation mask to train, we get a test accuracy of 76.2% (the training loss is 0.62)."]},{"cell_type":"markdown","metadata":{"id":"htlF1-6ys0fZ"},"source":["**Model with GCN with more layers and more feature dimensions**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K_ztZOlJtFvS","executionInfo":{"status":"ok","timestamp":1644081567938,"user_tz":-60,"elapsed":239,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"edf75075-c316-4e27-c25e-5d7fdcf7a2b6"},"source":["import torch\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv\n","\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self, hidden_channels1,hidden_channels2):\n","        super(GCN, self).__init__()\n","        torch.manual_seed(1234567)\n","        self.conv1 = GCNConv(dataset.num_features, hidden_channels1)\n","        self.conv2 = GCNConv(hidden_channels1, hidden_channels2)\n","        self.conv3 = GCNConv(hidden_channels2, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","        x = self.conv1(x, edge_index)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv3(x, edge_index)\n","        return x\n","\n","model = GCN(hidden_channels1=256,hidden_channels2=256)\n","\n","def model_summary(model):\n","    \n","    model_params_list = list(model.named_parameters())\n","    print(\"----------------------------------------------------------------\")\n","    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer.Parameter\", \"Param Tensor Shape\", \"Param #\")\n","    print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    for elem in model_params_list:\n","        p_name = elem[0] \n","        p_shape = list(elem[1].size())\n","        p_count = torch.tensor(elem[1].size()).prod().item()\n","        line_new = \"{:>20}  {:>25} {:>15}\".format(p_name, str(p_shape), str(p_count))\n","        print(line_new)\n","    print(\"----------------------------------------------------------------\")\n","    total_params = sum([param.nelement() for param in model.parameters()])\n","    print(\"Total params:\", total_params)\n","    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(\"Trainable params:\", num_trainable_params)\n","    print(\"Non-trainable params:\", total_params - num_trainable_params)\n","\n","model_summary(model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","     Layer.Parameter         Param Tensor Shape         Param #\n","----------------------------------------------------------------\n","          conv1.bias                      [256]             256\n","    conv1.lin.weight                [256, 3703]          947968\n","          conv2.bias                      [256]             256\n","    conv2.lin.weight                 [256, 256]           65536\n","          conv3.bias                        [6]               6\n","    conv3.lin.weight                   [6, 256]            1536\n","----------------------------------------------------------------\n","Total params: 1015558\n","Trainable params: 1015558\n","Non-trainable params: 0\n"]}]},{"cell_type":"code","metadata":{"id":"4MEHjGPsuY1K","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1644081654145,"user_tz":-60,"elapsed":79784,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"3677cc9b-dff2-4eeb-f89b-b5262ebf86e6"},"source":["from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","model = GCN(hidden_channels1=256,hidden_channels2=256)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss\n","\n","def test():\n","      model.eval()\n","      out = model(data.x, data.edge_index)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc\n","\n","\n","for epoch in range(1, 201):\n","    loss = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 1.7921\n","Epoch: 002, Loss: 1.7875\n","Epoch: 003, Loss: 1.7766\n","Epoch: 004, Loss: 1.7621\n","Epoch: 005, Loss: 1.7386\n","Epoch: 006, Loss: 1.6855\n","Epoch: 007, Loss: 1.6204\n","Epoch: 008, Loss: 1.5380\n","Epoch: 009, Loss: 1.4197\n","Epoch: 010, Loss: 1.2689\n","Epoch: 011, Loss: 1.1122\n","Epoch: 012, Loss: 0.9326\n","Epoch: 013, Loss: 0.7945\n","Epoch: 014, Loss: 0.6366\n","Epoch: 015, Loss: 0.5016\n","Epoch: 016, Loss: 0.4165\n","Epoch: 017, Loss: 0.3318\n","Epoch: 018, Loss: 0.2676\n","Epoch: 019, Loss: 0.2258\n","Epoch: 020, Loss: 0.2002\n","Epoch: 021, Loss: 0.1869\n","Epoch: 022, Loss: 0.1450\n","Epoch: 023, Loss: 0.1290\n","Epoch: 024, Loss: 0.1234\n","Epoch: 025, Loss: 0.1273\n","Epoch: 026, Loss: 0.1103\n","Epoch: 027, Loss: 0.0990\n","Epoch: 028, Loss: 0.0921\n","Epoch: 029, Loss: 0.1393\n","Epoch: 030, Loss: 0.0999\n","Epoch: 031, Loss: 0.1175\n","Epoch: 032, Loss: 0.0982\n","Epoch: 033, Loss: 0.1006\n","Epoch: 034, Loss: 0.0959\n","Epoch: 035, Loss: 0.0896\n","Epoch: 036, Loss: 0.0968\n","Epoch: 037, Loss: 0.0810\n","Epoch: 038, Loss: 0.0743\n","Epoch: 039, Loss: 0.0799\n","Epoch: 040, Loss: 0.0600\n","Epoch: 041, Loss: 0.0639\n","Epoch: 042, Loss: 0.0817\n","Epoch: 043, Loss: 0.0661\n","Epoch: 044, Loss: 0.0784\n","Epoch: 045, Loss: 0.1005\n","Epoch: 046, Loss: 0.0860\n","Epoch: 047, Loss: 0.0668\n","Epoch: 048, Loss: 0.0727\n","Epoch: 049, Loss: 0.0952\n","Epoch: 050, Loss: 0.0651\n","Epoch: 051, Loss: 0.0730\n","Epoch: 052, Loss: 0.0849\n","Epoch: 053, Loss: 0.0604\n","Epoch: 054, Loss: 0.0572\n","Epoch: 055, Loss: 0.0680\n","Epoch: 056, Loss: 0.0678\n","Epoch: 057, Loss: 0.0510\n","Epoch: 058, Loss: 0.0594\n","Epoch: 059, Loss: 0.0532\n","Epoch: 060, Loss: 0.0711\n","Epoch: 061, Loss: 0.0635\n","Epoch: 062, Loss: 0.0588\n","Epoch: 063, Loss: 0.0682\n","Epoch: 064, Loss: 0.0686\n","Epoch: 065, Loss: 0.0640\n","Epoch: 066, Loss: 0.0612\n","Epoch: 067, Loss: 0.0675\n","Epoch: 068, Loss: 0.0560\n","Epoch: 069, Loss: 0.0512\n","Epoch: 070, Loss: 0.0508\n","Epoch: 071, Loss: 0.0621\n","Epoch: 072, Loss: 0.0671\n","Epoch: 073, Loss: 0.0639\n","Epoch: 074, Loss: 0.0562\n","Epoch: 075, Loss: 0.0612\n","Epoch: 076, Loss: 0.0585\n","Epoch: 077, Loss: 0.0574\n","Epoch: 078, Loss: 0.0648\n","Epoch: 079, Loss: 0.0577\n","Epoch: 080, Loss: 0.0534\n","Epoch: 081, Loss: 0.0581\n","Epoch: 082, Loss: 0.0708\n","Epoch: 083, Loss: 0.0529\n","Epoch: 084, Loss: 0.0603\n","Epoch: 085, Loss: 0.0682\n","Epoch: 086, Loss: 0.0514\n","Epoch: 087, Loss: 0.0843\n","Epoch: 088, Loss: 0.0622\n","Epoch: 089, Loss: 0.0618\n","Epoch: 090, Loss: 0.0530\n","Epoch: 091, Loss: 0.0458\n","Epoch: 092, Loss: 0.0510\n","Epoch: 093, Loss: 0.0588\n","Epoch: 094, Loss: 0.0548\n","Epoch: 095, Loss: 0.0578\n","Epoch: 096, Loss: 0.0505\n","Epoch: 097, Loss: 0.0625\n","Epoch: 098, Loss: 0.0511\n","Epoch: 099, Loss: 0.0607\n","Epoch: 100, Loss: 0.0523\n","Epoch: 101, Loss: 0.0656\n","Epoch: 102, Loss: 0.0504\n","Epoch: 103, Loss: 0.0612\n","Epoch: 104, Loss: 0.0537\n","Epoch: 105, Loss: 0.0543\n","Epoch: 106, Loss: 0.0691\n","Epoch: 107, Loss: 0.0522\n","Epoch: 108, Loss: 0.0627\n","Epoch: 109, Loss: 0.0649\n","Epoch: 110, Loss: 0.0788\n","Epoch: 111, Loss: 0.0627\n","Epoch: 112, Loss: 0.0447\n","Epoch: 113, Loss: 0.0801\n","Epoch: 114, Loss: 0.0517\n","Epoch: 115, Loss: 0.0581\n","Epoch: 116, Loss: 0.0752\n","Epoch: 117, Loss: 0.0482\n","Epoch: 118, Loss: 0.0503\n","Epoch: 119, Loss: 0.0579\n","Epoch: 120, Loss: 0.0400\n","Epoch: 121, Loss: 0.0403\n","Epoch: 122, Loss: 0.0583\n","Epoch: 123, Loss: 0.0457\n","Epoch: 124, Loss: 0.0557\n","Epoch: 125, Loss: 0.0502\n","Epoch: 126, Loss: 0.0475\n","Epoch: 127, Loss: 0.0560\n","Epoch: 128, Loss: 0.0449\n","Epoch: 129, Loss: 0.0584\n","Epoch: 130, Loss: 0.0499\n","Epoch: 131, Loss: 0.0693\n","Epoch: 132, Loss: 0.0447\n","Epoch: 133, Loss: 0.0746\n","Epoch: 134, Loss: 0.0422\n","Epoch: 135, Loss: 0.0453\n","Epoch: 136, Loss: 0.0414\n","Epoch: 137, Loss: 0.0470\n","Epoch: 138, Loss: 0.0517\n","Epoch: 139, Loss: 0.0508\n","Epoch: 140, Loss: 0.0546\n","Epoch: 141, Loss: 0.0391\n","Epoch: 142, Loss: 0.0496\n","Epoch: 143, Loss: 0.0430\n","Epoch: 144, Loss: 0.0510\n","Epoch: 145, Loss: 0.0451\n","Epoch: 146, Loss: 0.0489\n","Epoch: 147, Loss: 0.0451\n","Epoch: 148, Loss: 0.0615\n","Epoch: 149, Loss: 0.0447\n","Epoch: 150, Loss: 0.0458\n","Epoch: 151, Loss: 0.0522\n","Epoch: 152, Loss: 0.0442\n","Epoch: 153, Loss: 0.0487\n","Epoch: 154, Loss: 0.0428\n","Epoch: 155, Loss: 0.0416\n","Epoch: 156, Loss: 0.0555\n","Epoch: 157, Loss: 0.0419\n","Epoch: 158, Loss: 0.0392\n","Epoch: 159, Loss: 0.0459\n","Epoch: 160, Loss: 0.0409\n","Epoch: 161, Loss: 0.0447\n","Epoch: 162, Loss: 0.0401\n","Epoch: 163, Loss: 0.0368\n","Epoch: 164, Loss: 0.0461\n","Epoch: 165, Loss: 0.0403\n","Epoch: 166, Loss: 0.0486\n","Epoch: 167, Loss: 0.0432\n","Epoch: 168, Loss: 0.0442\n","Epoch: 169, Loss: 0.0359\n","Epoch: 170, Loss: 0.0534\n","Epoch: 171, Loss: 0.0597\n","Epoch: 172, Loss: 0.0424\n","Epoch: 173, Loss: 0.0435\n","Epoch: 174, Loss: 0.0449\n","Epoch: 175, Loss: 0.0462\n","Epoch: 176, Loss: 0.0448\n","Epoch: 177, Loss: 0.0444\n","Epoch: 178, Loss: 0.0432\n","Epoch: 179, Loss: 0.0489\n","Epoch: 180, Loss: 0.0348\n","Epoch: 181, Loss: 0.0429\n","Epoch: 182, Loss: 0.0386\n","Epoch: 183, Loss: 0.0450\n","Epoch: 184, Loss: 0.0479\n","Epoch: 185, Loss: 0.0422\n","Epoch: 186, Loss: 0.0651\n","Epoch: 187, Loss: 0.0396\n","Epoch: 188, Loss: 0.0363\n","Epoch: 189, Loss: 0.0558\n","Epoch: 190, Loss: 0.0409\n","Epoch: 191, Loss: 0.0566\n","Epoch: 192, Loss: 0.0387\n","Epoch: 193, Loss: 0.0609\n","Epoch: 194, Loss: 0.0317\n","Epoch: 195, Loss: 0.0452\n","Epoch: 196, Loss: 0.0517\n","Epoch: 197, Loss: 0.0410\n","Epoch: 198, Loss: 0.0393\n","Epoch: 199, Loss: 0.0425\n","Epoch: 200, Loss: 0.0424\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"802Ei52SvLq0","executionInfo":{"status":"ok","timestamp":1634417222767,"user_tz":-120,"elapsed":301,"user":{"displayName":"Adrian Hernandez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjcBgwUm3IOIr2eMtDO7J2Z0-Ezy0aPy2YzerW1FA=s64","userId":"05338976519150686624"}},"outputId":"0ad4931f-8a3e-4625-f968-63d7b2dc0a96"},"source":["test_acc = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7330\n"]}]},{"cell_type":"markdown","metadata":{"id":"rw3MPTdxxiHO"},"source":["Using a GNN with two intermediate layer we see that while increasing the feature dimensions the test accuracy decreases to 62%, probably because the model is overfitting the training data (the training loss is 0.04 aprox). If we lower the feature dimensions to 16, then the test accuracy returns to 67.4%. If we use the validation mask to train the model, we get a test accuracy of 71.5% or 73.3% with feature dimensions of 256 (the training loss is 0.07)."]}]}